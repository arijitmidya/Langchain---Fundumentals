{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ingestion- Documentloaders\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\ariji\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (0.3.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (0.1.129)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (1.24.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain_community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a text file - TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x1b3b5ef7f10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Reading a text file\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader('C:/Users/ariji/Exploratory Data Analysis/speech.txt')\n",
    "loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(text_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our heartsâ€”for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_document[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'C:/Users/ariji/Exploratory Data Analysis/speech.txt'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_document[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a PDF - PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a PDF file \n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('C:/Users/ariji/Exploratory Data Analysis/attention.pdf')\n",
    "pdf_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "======================================================================================================================================================\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [ 21] and conditional\n",
      "computation [ 32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n",
      "of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2\n",
      "======================================================================================================================================================\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(pdf_docs))\n",
    "print(pdf_docs[0].page_content)\n",
    "print('='*150)\n",
    "print(pdf_docs[1].page_content)\n",
    "print('='*150)\n",
    "print(pdf_docs[14].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading an web based document - WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web based loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                       ))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(web_path=(\"https://paulgraham.com/greatwork.html\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://paulgraham.com/greatwork.html', 'title': 'How to Do Great Work', 'language': 'No language found.'}, page_content='How to Do Great Work\\n\\nJuly 2023If you collected lists of techniques for doing great work in a lot\\nof different fields, what would the intersection look like? I decided\\nto find out by making it.Partly my goal was to create a guide that could be used by someone\\nworking in any field. But I was also curious about the shape of the\\nintersection. And one thing this exercise shows is that it does\\nhave a definite shape; it\\'s not just a point labelled \"work hard.\"The following recipe assumes you\\'re very ambitious.\\nThe first step is to decide what to work on. The work you choose\\nneeds to have three qualities: it has to be something you have a\\nnatural aptitude for, that you have a deep interest in, and that\\noffers scope to do great work.In practice you don\\'t have to worry much about the third criterion.\\nAmbitious people are if anything already too conservative about it.\\nSo all you need to do is find something you have an aptitude for\\nand great interest in.\\n[1]That sounds straightforward, but it\\'s often quite difficult. When\\nyou\\'re young you don\\'t know what you\\'re good at or what different\\nkinds of work are like. Some kinds of work you end up doing may not\\neven exist yet. So while some people know what they want to do at\\n14, most have to figure it out.The way to figure out what to work on is by working. If you\\'re not\\nsure what to work on, guess. But pick something and get going.\\nYou\\'ll probably guess wrong some of the time, but that\\'s fine. It\\'s\\ngood to know about multiple things; some of the biggest discoveries\\ncome from noticing connections between different fields.Develop a habit of working on your own projects. Don\\'t let \"work\"\\nmean something other people tell you to do. If you do manage to do\\ngreat work one day, it will probably be on a project of your own.\\nIt may be within some bigger project, but you\\'ll be driving your\\npart of it.What should your projects be? Whatever seems to you excitingly\\nambitious. As you grow older and your taste in projects evolves,\\nexciting and important will converge. At 7 it may seem excitingly\\nambitious to build huge things out of Lego, then at 14 to teach\\nyourself calculus, till at 21 you\\'re starting to explore unanswered\\nquestions in physics. But always preserve excitingness.There\\'s a kind of excited curiosity that\\'s both the engine and the\\nrudder of great work. It will not only drive you, but if you let\\nit have its way, will also show you what to work on.What are you excessively curious about — curious to a degree that\\nwould bore most other people? That\\'s what you\\'re looking for.Once you\\'ve found something you\\'re excessively interested in, the\\nnext step is to learn enough about it to get you to one of the\\nfrontiers of knowledge. Knowledge expands fractally, and from a\\ndistance its edges look smooth, but once you learn enough to get\\nclose to one, they turn out to be full of gaps.The next step is to notice them. This takes some skill, because\\nyour brain wants to ignore such gaps in order to make a simpler\\nmodel of the world. Many discoveries have come from asking questions\\nabout things that everyone else took for granted. \\n[2]If the answers seem strange, so much the better. Great work often\\nhas a tincture of strangeness. You see this from painting to math.\\nIt would be affected to try to manufacture it, but if it appears,\\nembrace it.Boldly chase outlier ideas, even if other people aren\\'t interested\\nin them — in fact, especially if they aren\\'t. If you\\'re excited\\nabout some possibility that everyone else ignores, and you have\\nenough expertise to say precisely what they\\'re all overlooking,\\nthat\\'s as good a bet as you\\'ll find.\\n[3]Four steps: choose a field, learn enough to get to the frontier,\\nnotice gaps, explore promising ones. This is how practically everyone\\nwho\\'s done great work has done it, from painters to physicists.Steps two and four will require hard work. It may not be possible\\nto prove that you have to work hard to do great things, but the\\nempirical evidence is on the scale of the evidence for mortality.\\nThat\\'s why it\\'s essential to work on something you\\'re deeply\\ninterested in. Interest will drive you to work harder than mere\\ndiligence ever could.The three most powerful motives are curiosity, delight, and the\\ndesire to do something impressive. Sometimes they converge, and\\nthat combination is the most powerful of all.The big prize is to discover a new fractal bud. You notice a crack\\nin the surface of knowledge, pry it open, and there\\'s a whole world\\ninside.Let\\'s talk a little more about the complicated business of figuring\\nout what to work on. The main reason it\\'s hard is that you can\\'t\\ntell what most kinds of work are like except by doing them. Which\\nmeans the four steps overlap: you may have to work at something for\\nyears before you know how much you like it or how good you are at\\nit. And in the meantime you\\'re not doing, and thus not learning\\nabout, most other kinds of work. So in the worst case you choose\\nlate based on very incomplete information.\\n[4]The nature of ambition exacerbates this problem. Ambition comes in\\ntwo forms, one that precedes interest in the subject and one that\\ngrows out of it. Most people who do great work have a mix, and the\\nmore you have of the former, the harder it will be to decide what\\nto do.The educational systems in most countries pretend it\\'s easy. They\\nexpect you to commit to a field long before you could know what\\nit\\'s really like. And as a result an ambitious person on an optimal\\ntrajectory will often read to the system as an instance of breakage.It would be better if they at least admitted it — if they admitted\\nthat the system not only can\\'t do much to help you figure out what\\nto work on, but is designed on the assumption that you\\'ll somehow\\nmagically guess as a teenager. They don\\'t tell you, but I will:\\nwhen it comes to figuring out what to work on, you\\'re on your own.\\nSome people get lucky and do guess correctly, but the rest will\\nfind themselves scrambling diagonally across tracks laid down on\\nthe assumption that everyone does.What should you do if you\\'re young and ambitious but don\\'t know\\nwhat to work on? What you should not do is drift along passively,\\nassuming the problem will solve itself. You need to take action.\\nBut there is no systematic procedure you can follow. When you read\\nbiographies of people who\\'ve done great work, it\\'s remarkable how\\nmuch luck is involved. They discover what to work on as a result\\nof a chance meeting, or by reading a book they happen to pick up.\\nSo you need to make yourself a big target for luck, and the way to\\ndo that is to be curious. Try lots of things, meet lots of people,\\nread lots of books, ask lots of questions.\\n[5]When in doubt, optimize for interestingness. Fields change as you\\nlearn more about them. What mathematicians do, for example, is very\\ndifferent from what you do in high school math classes. So you need\\nto give different types of work a chance to show you what they\\'re\\nlike. But a field should become increasingly interesting as you\\nlearn more about it. If it doesn\\'t, it\\'s probably not for you.Don\\'t worry if you find you\\'re interested in different things than\\nother people. The stranger your tastes in interestingness, the\\nbetter. Strange tastes are often strong ones, and a strong taste\\nfor work means you\\'ll be productive. And you\\'re more likely to find\\nnew things if you\\'re looking where few have looked before.One sign that you\\'re suited for some kind of work is when you like\\neven the parts that other people find tedious or frightening.But fields aren\\'t people; you don\\'t owe them any loyalty. If in the\\ncourse of working on one thing you discover another that\\'s more\\nexciting, don\\'t be afraid to switch.If you\\'re making something for people, make sure it\\'s something\\nthey actually want. The best way to do this is to make something\\nyou yourself want. Write the story you want to read; build the tool\\nyou want to use. Since your friends probably have similar interests,\\nthis will also get you your initial audience.This should follow from the excitingness rule. Obviously the most\\nexciting story to write will be the one you want to read. The reason\\nI mention this case explicitly is that so many people get it wrong.\\nInstead of making what they want, they try to make what some\\nimaginary, more sophisticated audience wants. And once you go down\\nthat route, you\\'re lost.\\n[6]There are a lot of forces that will lead you astray when you\\'re\\ntrying to figure out what to work on. Pretentiousness, fashion,\\nfear, money, politics, other people\\'s wishes, eminent frauds. But\\nif you stick to what you find genuinely interesting, you\\'ll be proof\\nagainst all of them. If you\\'re interested, you\\'re not astray.\\nFollowing your interests may sound like a rather passive strategy,\\nbut in practice it usually means following them past all sorts of\\nobstacles. You usually have to risk rejection and failure. So it\\ndoes take a good deal of boldness.But while you need boldness, you don\\'t usually need much planning.\\nIn most cases the recipe for doing great work is simply: work hard\\non excitingly ambitious projects, and something good will come of\\nit. Instead of making a plan and then executing it, you just try\\nto preserve certain invariants.The trouble with planning is that it only works for achievements\\nyou can describe in advance. You can win a gold medal or get rich\\nby deciding to as a child and then tenaciously pursuing that goal,\\nbut you can\\'t discover natural selection that way.I think for most people who want to do great work, the right strategy\\nis not to plan too much. At each stage do whatever seems most\\ninteresting and gives you the best options for the future. I call\\nthis approach \"staying upwind.\" This is how most people who\\'ve done\\ngreat work seem to have done it.\\nEven when you\\'ve found something exciting to work on, working on\\nit is not always straightforward. There will be times when some new\\nidea makes you leap out of bed in the morning and get straight to\\nwork. But there will also be plenty of times when things aren\\'t\\nlike that.You don\\'t just put out your sail and get blown forward by inspiration.\\nThere are headwinds and currents and hidden shoals. So there\\'s a\\ntechnique to working, just as there is to sailing.For example, while you must work hard, it\\'s possible to work too\\nhard, and if you do that you\\'ll find you get diminishing returns:\\nfatigue will make you stupid, and eventually even damage your health.\\nThe point at which work yields diminishing returns depends on the\\ntype. Some of the hardest types you might only be able to do for\\nfour or five hours a day.Ideally those hours will be contiguous. To the extent you can, try\\nto arrange your life so you have big blocks of time to work in.\\nYou\\'ll shy away from hard tasks if you know you might be interrupted.It will probably be harder to start working than to keep working.\\nYou\\'ll often have to trick yourself to get over that initial\\nthreshold. Don\\'t worry about this; it\\'s the nature of work, not a\\nflaw in your character. Work has a sort of activation energy, both\\nper day and per project. And since this threshold is fake in the\\nsense that it\\'s higher than the energy required to keep going, it\\'s\\nok to tell yourself a lie of corresponding magnitude to get over\\nit.It\\'s usually a mistake to lie to yourself if you want to do great\\nwork, but this is one of the rare cases where it isn\\'t. When I\\'m\\nreluctant to start work in the morning, I often trick myself by\\nsaying \"I\\'ll just read over what I\\'ve got so far.\" Five minutes\\nlater I\\'ve found something that seems mistaken or incomplete, and\\nI\\'m off.Similar techniques work for starting new projects. It\\'s ok to lie\\nto yourself about how much work a project will entail, for example.\\nLots of great things began with someone saying \"How hard could it\\nbe?\"This is one case where the young have an advantage. They\\'re more\\noptimistic, and even though one of the sources of their optimism\\nis ignorance, in this case ignorance can sometimes beat knowledge.Try to finish what you start, though, even if it turns out to be\\nmore work than you expected. Finishing things is not just an exercise\\nin tidiness or self-discipline. In many projects a lot of the best\\nwork happens in what was meant to be the final stage.Another permissible lie is to exaggerate the importance of what\\nyou\\'re working on, at least in your own mind. If that helps you\\ndiscover something new, it may turn out not to have been a lie after\\nall.\\n[7]\\nSince there are two senses of starting work — per day and per\\nproject — there are also two forms of procrastination. Per-project\\nprocrastination is far the more dangerous. You put off starting\\nthat ambitious project from year to year because the time isn\\'t\\nquite right. When you\\'re procrastinating in units of years, you can\\nget a lot not done.\\n[8]One reason per-project procrastination is so dangerous is that it\\nusually camouflages itself as work. You\\'re not just sitting around\\ndoing nothing; you\\'re working industriously on something else. So\\nper-project procrastination doesn\\'t set off the alarms that per-day\\nprocrastination does. You\\'re too busy to notice it.The way to beat it is to stop occasionally and ask yourself: Am I\\nworking on what I most want to work on? When you\\'re young it\\'s ok\\nif the answer is sometimes no, but this gets increasingly dangerous\\nas you get older.\\n[9]\\nGreat work usually entails spending what would seem to most people\\nan unreasonable amount of time on a problem. You can\\'t think of\\nthis time as a cost, or it will seem too high. You have to find the\\nwork sufficiently engaging as it\\'s happening.There may be some jobs where you have to work diligently for years\\nat things you hate before you get to the good part, but this is not\\nhow great work happens. Great work happens by focusing consistently\\non something you\\'re genuinely interested in. When you pause to take\\nstock, you\\'re surprised how far you\\'ve come.The reason we\\'re surprised is that we underestimate the cumulative\\neffect of work. Writing a page a day doesn\\'t sound like much, but\\nif you do it every day you\\'ll write a book a year. That\\'s the key:\\nconsistency. People who do great things don\\'t get a lot done every\\nday. They get something done, rather than nothing.If you do work that compounds, you\\'ll get exponential growth. Most\\npeople who do this do it unconsciously, but it\\'s worth stopping to\\nthink about. Learning, for example, is an instance of this phenomenon:\\nthe more you learn about something, the easier it is to learn more.\\nGrowing an audience is another: the more fans you have, the more\\nnew fans they\\'ll bring you.The trouble with exponential growth is that the curve feels flat\\nin the beginning. It isn\\'t; it\\'s still a wonderful exponential\\ncurve. But we can\\'t grasp that intuitively, so we underrate exponential\\ngrowth in its early stages.Something that grows exponentially can become so valuable that it\\'s\\nworth making an extraordinary effort to get it started. But since\\nwe underrate exponential growth early on, this too is mostly done\\nunconsciously: people push through the initial, unrewarding phase\\nof learning something new because they know from experience that\\nlearning new things always takes an initial push, or they grow their\\naudience one fan at a time because they have nothing better to do.\\nIf people consciously realized they could invest in exponential\\ngrowth, many more would do it.\\nWork doesn\\'t just happen when you\\'re trying to. There\\'s a kind of\\nundirected thinking you do when walking or taking a shower or lying\\nin bed that can be very powerful. By letting your mind wander a\\nlittle, you\\'ll often solve problems you were unable to solve by\\nfrontal attack.You have to be working hard in the normal way to benefit from this\\nphenomenon, though. You can\\'t just walk around daydreaming. The\\ndaydreaming has to be interleaved with deliberate work that feeds\\nit questions.\\n[10]Everyone knows to avoid distractions at work, but it\\'s also important\\nto avoid them in the other half of the cycle. When you let your\\nmind wander, it wanders to whatever you care about most at that\\nmoment. So avoid the kind of distraction that pushes your work out\\nof the top spot, or you\\'ll waste this valuable type of thinking on\\nthe distraction instead. (Exception: Don\\'t avoid love.)\\nConsciously cultivate your taste in the work done in your field.\\nUntil you know which is the best and what makes it so, you don\\'t\\nknow what you\\'re aiming for.And that is what you\\'re aiming for, because if you don\\'t try to\\nbe the best, you won\\'t even be good. This observation has been made\\nby so many people in so many different fields that it might be worth\\nthinking about why it\\'s true. It could be because ambition is a\\nphenomenon where almost all the error is in one direction — where\\nalmost all the shells that miss the target miss by falling short.\\nOr it could be because ambition to be the best is a qualitatively\\ndifferent thing from ambition to be good. Or maybe being good is\\nsimply too vague a standard. Probably all three are true.\\n[11]Fortunately there\\'s a kind of economy of scale here. Though it might\\nseem like you\\'d be taking on a heavy burden by trying to be the\\nbest, in practice you often end up net ahead. It\\'s exciting, and\\nalso strangely liberating. It simplifies things. In some ways it\\'s\\neasier to try to be the best than to try merely to be good.One way to aim high is to try to make something that people will\\ncare about in a hundred years. Not because their opinions matter\\nmore than your contemporaries\\', but because something that still\\nseems good in a hundred years is more likely to be genuinely good.\\nDon\\'t try to work in a distinctive style. Just try to do the best\\njob you can; you won\\'t be able to help doing it in a distinctive\\nway.Style is doing things in a distinctive way without trying to. Trying\\nto is affectation.Affectation is in effect to pretend that someone other than you is\\ndoing the work. You adopt an impressive but fake persona, and while\\nyou\\'re pleased with the impressiveness, the fakeness is what shows\\nin the work.\\n[12]The temptation to be someone else is greatest for the young. They\\noften feel like nobodies. But you never need to worry about that\\nproblem, because it\\'s self-solving if you work on sufficiently\\nambitious projects. If you succeed at an ambitious project, you\\'re\\nnot a nobody; you\\'re the person who did it. So just do the work and\\nyour identity will take care of itself.\\n\"Avoid affectation\" is a useful rule so far as it goes, but how\\nwould you express this idea positively? How would you say what to\\nbe, instead of what not to be? The best answer is earnest. If you\\'re\\nearnest you avoid not just affectation but a whole set of similar\\nvices.The core of being earnest is being intellectually honest. We\\'re\\ntaught as children to be honest as an unselfish virtue — as a kind\\nof sacrifice. But in fact it\\'s a source of power too. To see new\\nideas, you need an exceptionally sharp eye for the truth. You\\'re\\ntrying to see more truth than others have seen so far. And how can\\nyou have a sharp eye for the truth if you\\'re intellectually dishonest?One way to avoid intellectual dishonesty is to maintain a slight\\npositive pressure in the opposite direction. Be aggressively willing\\nto admit that you\\'re mistaken. Once you\\'ve admitted you were mistaken\\nabout something, you\\'re free. Till then you have to carry it.\\n[13]Another more subtle component of earnestness is informality.\\nInformality is much more important than its grammatically negative\\nname implies. It\\'s not merely the absence of something. It means\\nfocusing on what matters instead of what doesn\\'t.What formality and affectation have in common is that as well as\\ndoing the work, you\\'re trying to seem a certain way as you\\'re doing\\nit. But any energy that goes into how you seem comes out of being\\ngood. That\\'s one reason nerds have an advantage in doing great work:\\nthey expend little effort on seeming anything. In fact that\\'s\\nbasically the definition of a nerd.Nerds have a kind of innocent boldness that\\'s exactly what you need\\nin doing great work. It\\'s not learned; it\\'s preserved from childhood.\\nSo hold onto it. Be the one who puts things out there rather than\\nthe one who sits back and offers sophisticated-sounding criticisms\\nof them. \"It\\'s easy to criticize\" is true in the most literal sense,\\nand the route to great work is never easy.There may be some jobs where it\\'s an advantage to be cynical and\\npessimistic, but if you want to do great work it\\'s an advantage to\\nbe optimistic, even though that means you\\'ll risk looking like a\\nfool sometimes. There\\'s an old tradition of doing the opposite. The\\nOld Testament says it\\'s better to keep quiet lest you look like a\\nfool. But that\\'s advice for seeming smart. If you actually want\\nto discover new things, it\\'s better to take the risk of telling\\npeople your ideas.Some people are naturally earnest, and with others it takes a\\nconscious effort. Either kind of earnestness will suffice. But I\\ndoubt it would be possible to do great work without being earnest.\\nIt\\'s so hard to do even if you are. You don\\'t have enough margin\\nfor error to accommodate the distortions introduced by being affected,\\nintellectually dishonest, orthodox, fashionable, or cool.\\n[14]\\nGreat work is consistent not only with who did it, but with itself.\\nIt\\'s usually all of a piece. So if you face a decision in the middle\\nof working on something, ask which choice is more consistent.You may have to throw things away and redo them. You won\\'t necessarily\\nhave to, but you have to be willing to. And that can take some\\neffort; when there\\'s something you need to redo, status quo bias\\nand laziness will combine to keep you in denial about it. To beat\\nthis ask: If I\\'d already made the change, would I want to revert\\nto what I have now?Have the confidence to cut. Don\\'t keep something that doesn\\'t fit\\njust because you\\'re proud of it, or because it cost you a lot of\\neffort.Indeed, in some kinds of work it\\'s good to strip whatever you\\'re\\ndoing to its essence. The result will be more concentrated; you\\'ll\\nunderstand it better; and you won\\'t be able to lie to yourself about\\nwhether there\\'s anything real there.Mathematical elegance may sound like a mere metaphor, drawn from\\nthe arts. That\\'s what I thought when I first heard the term \"elegant\"\\napplied to a proof. But now I suspect it\\'s conceptually prior — \\nthat the main ingredient in artistic elegance is mathematical\\nelegance. At any rate it\\'s a useful standard well beyond math.Elegance can be a long-term bet, though. Laborious solutions will\\noften have more prestige in the short term. They cost a lot of\\neffort and they\\'re hard to understand, both of which impress people,\\nat least temporarily.Whereas some of the very best work will seem like it took comparatively\\nlittle effort, because it was in a sense already there. It didn\\'t\\nhave to be built, just seen. It\\'s a very good sign when it\\'s hard\\nto say whether you\\'re creating something or discovering it.When you\\'re doing work that could be seen as either creation or\\ndiscovery, err on the side of discovery. Try thinking of yourself\\nas a mere conduit through which the ideas take their natural shape.(Strangely enough, one exception is the problem of choosing a problem\\nto work on. This is usually seen as search, but in the best case\\nit\\'s more like creating something. In the best case you create the\\nfield in the process of exploring it.)Similarly, if you\\'re trying to build a powerful tool, make it\\ngratuitously unrestrictive. A powerful tool almost by definition\\nwill be used in ways you didn\\'t expect, so err on the side of\\neliminating restrictions, even if you don\\'t know what the benefit\\nwill be.Great work will often be tool-like in the sense of being something\\nothers build on. So it\\'s a good sign if you\\'re creating ideas that\\nothers could use, or exposing questions that others could answer.\\nThe best ideas have implications in many different areas.If you express your ideas in the most general form, they\\'ll be truer\\nthan you intended.\\nTrue by itself is not enough, of course. Great ideas have to be\\ntrue and new. And it takes a certain amount of ability to see new\\nideas even once you\\'ve learned enough to get to one of the frontiers\\nof knowledge.In English we give this ability names like originality, creativity,\\nand imagination. And it seems reasonable to give it a separate name,\\nbecause it does seem to some extent a separate skill. It\\'s possible\\nto have a great deal of ability in other respects — to have a great\\ndeal of what\\'s often called technical ability — and yet not have\\nmuch of this.I\\'ve never liked the term \"creative process.\" It seems misleading.\\nOriginality isn\\'t a process, but a habit of mind. Original thinkers\\nthrow off new ideas about whatever they focus on, like an angle\\ngrinder throwing off sparks. They can\\'t help it.If the thing they\\'re focused on is something they don\\'t understand\\nvery well, these new ideas might not be good. One of the most\\noriginal thinkers I know decided to focus on dating after he got\\ndivorced. He knew roughly as much about dating as the average 15\\nyear old, and the results were spectacularly colorful. But to see\\noriginality separated from expertise like that made its nature all\\nthe more clear.I don\\'t know if it\\'s possible to cultivate originality, but there\\nare definitely ways to make the most of however much you have. For\\nexample, you\\'re much more likely to have original ideas when you\\'re\\nworking on something. Original ideas don\\'t come from trying to have\\noriginal ideas. They come from trying to build or understand something\\nslightly too difficult.\\n[15]Talking or writing about the things you\\'re interested in is a good\\nway to generate new ideas. When you try to put ideas into words, a\\nmissing idea creates a sort of vacuum that draws it out of you.\\nIndeed, there\\'s a kind of thinking that can only be done by writing.Changing your context can help. If you visit a new place, you\\'ll\\noften find you have new ideas there. The journey itself often\\ndislodges them. But you may not have to go far to get this benefit.\\nSometimes it\\'s enough just to go for a walk.\\n[16]It also helps to travel in topic space. You\\'ll have more new ideas\\nif you explore lots of different topics, partly because it gives\\nthe angle grinder more surface area to work on, and partly because\\nanalogies are an especially fruitful source of new ideas.Don\\'t divide your attention evenly between many topics though,\\nor you\\'ll spread yourself too thin. You want to distribute it\\naccording to something more like a power law.\\n[17]\\nBe professionally\\ncurious about a few topics and idly curious about many more.Curiosity and originality are closely related. Curiosity feeds\\noriginality by giving it new things to work on. But the relationship\\nis closer than that. Curiosity is itself a kind of originality;\\nit\\'s roughly to questions what originality is to answers. And since\\nquestions at their best are a big component of answers, curiosity\\nat its best is a creative force.\\nHaving new ideas is a strange game, because it usually consists of\\nseeing things that were right under your nose. Once you\\'ve seen a\\nnew idea, it tends to seem obvious. Why did no one think of this\\nbefore?When an idea seems simultaneously novel and obvious, it\\'s probably\\na good one.Seeing something obvious sounds easy. And yet empirically having\\nnew ideas is hard. What\\'s the source of this apparent contradiction?\\nIt\\'s that seeing the new idea usually requires you to change the\\nway you look at the world. We see the world through models that\\nboth help and constrain us. When you fix a broken model, new ideas\\nbecome obvious. But noticing and fixing a broken model is hard.\\nThat\\'s how new ideas can be both obvious and yet hard to discover:\\nthey\\'re easy to see after you do something hard.One way to discover broken models is to be stricter than other\\npeople. Broken models of the world leave a trail of clues where\\nthey bash against reality. Most people don\\'t want to see these\\nclues. It would be an understatement to say that they\\'re attached\\nto their current model; it\\'s what they think in; so they\\'ll tend\\nto ignore the trail of clues left by its breakage, however conspicuous\\nit may seem in retrospect.To find new ideas you have to seize on signs of breakage instead\\nof looking away. That\\'s what Einstein did. He was able to see the\\nwild implications of Maxwell\\'s equations not so much because he was\\nlooking for new ideas as because he was stricter.The other thing you need is a willingness to break rules. Paradoxical\\nas it sounds, if you want to fix your model of the world, it helps\\nto be the sort of person who\\'s comfortable breaking rules. From the\\npoint of view of the old model, which everyone including you initially\\nshares, the new model usually breaks at least implicit rules.Few understand the degree of rule-breaking required, because new\\nideas seem much more conservative once they succeed. They seem\\nperfectly reasonable once you\\'re using the new model of the world\\nthey brought with them. But they didn\\'t at the time; it took the\\ngreater part of a century for the heliocentric model to be generally\\naccepted, even among astronomers, because it felt so wrong.Indeed, if you think about it, a good new idea has to seem bad to\\nmost people, or someone would have already explored it. So what\\nyou\\'re looking for is ideas that seem crazy, but the right kind of\\ncrazy. How do you recognize these? You can\\'t with certainty. Often\\nideas that seem bad are bad. But ideas that are the right kind of\\ncrazy tend to be exciting; they\\'re rich in implications; whereas\\nideas that are merely bad tend to be depressing.There are two ways to be comfortable breaking rules: to enjoy\\nbreaking them, and to be indifferent to them. I call these two cases\\nbeing aggressively and passively independent-minded.The aggressively independent-minded are the naughty ones. Rules\\ndon\\'t merely fail to stop them; breaking rules gives them additional\\nenergy. For this sort of person, delight at the sheer audacity of\\na project sometimes supplies enough activation energy to get it\\nstarted.The other way to break rules is not to care about them, or perhaps\\neven to know they exist. This is why novices and outsiders often\\nmake new discoveries; their ignorance of a field\\'s assumptions acts\\nas a source of temporary passive independent-mindedness. Aspies\\nalso seem to have a kind of immunity to conventional beliefs.\\nSeveral I know say that this helps them to have new ideas.Strictness plus rule-breaking sounds like a strange combination.\\nIn popular culture they\\'re opposed. But popular culture has a broken\\nmodel in this respect. It implicitly assumes that issues are trivial\\nones, and in trivial matters strictness and rule-breaking are\\nopposed. But in questions that really matter, only rule-breakers\\ncan be truly strict.\\nAn overlooked idea often doesn\\'t lose till the semifinals. You do\\nsee it, subconsciously, but then another part of your subconscious\\nshoots it down because it would be too weird, too risky, too much\\nwork, too controversial. This suggests an exciting possibility: if\\nyou could turn off such filters, you could see more new ideas.One way to do that is to ask what would be good ideas for someone\\nelse to explore. Then your subconscious won\\'t shoot them down to\\nprotect you.You could also discover overlooked ideas by working in the other\\ndirection: by starting from what\\'s obscuring them. Every cherished\\nbut mistaken principle is surrounded by a dead zone of valuable\\nideas that are unexplored because they contradict it.Religions are collections of cherished but mistaken principles. So\\nanything that can be described either literally or metaphorically\\nas a religion will have valuable unexplored ideas in its shadow.\\nCopernicus and Darwin both made discoveries of this type.\\n[18]What are people in your field religious about, in the sense of being\\ntoo attached to some principle that might not be as self-evident\\nas they think? What becomes possible if you discard it?\\nPeople show much more originality in solving problems than in\\ndeciding which problems to solve. Even the smartest can be surprisingly\\nconservative when deciding what to work on. People who\\'d never dream\\nof being fashionable in any other way get sucked into working on\\nfashionable problems.One reason people are more conservative when choosing problems than\\nsolutions is that problems are bigger bets. A problem could occupy\\nyou for years, while exploring a solution might only take days. But\\neven so I think most people are too conservative. They\\'re not merely\\nresponding to risk, but to fashion as well. Unfashionable problems\\nare undervalued.One of the most interesting kinds of unfashionable problem is the\\nproblem that people think has been fully explored, but hasn\\'t.\\nGreat work often takes something that already exists and shows its\\nlatent potential. Durer and Watt both did this. So if you\\'re\\ninterested in a field that others think is tapped out, don\\'t let\\ntheir skepticism deter you. People are often wrong about this.Working on an unfashionable problem can be very pleasing. There\\'s\\nno hype or hurry. Opportunists and critics are both occupied\\nelsewhere. The existing work often has an old-school solidity. And\\nthere\\'s a satisfying sense of economy in cultivating ideas that\\nwould otherwise be wasted.But the most common type of overlooked problem is not explicitly\\nunfashionable in the sense of being out of fashion. It just doesn\\'t\\nseem to matter as much as it actually does. How do you find these?\\nBy being self-indulgent — by letting your curiosity have its way,\\nand tuning out, at least temporarily, the little voice in your head\\nthat says you should only be working on \"important\" problems.You do need to work on important problems, but almost everyone is\\ntoo conservative about what counts as one. And if there\\'s an important\\nbut overlooked problem in your neighborhood, it\\'s probably already\\non your subconscious radar screen. So try asking yourself: if you\\nwere going to take a break from \"serious\" work to work on something\\njust because it would be really interesting, what would you do? The\\nanswer is probably more important than it seems.Originality in choosing problems seems to matter even more than\\noriginality in solving them. That\\'s what distinguishes the people\\nwho discover whole new fields. So what might seem to be merely the\\ninitial step — deciding what to work on — is in a sense the key\\nto the whole game.\\nFew grasp this. One of the biggest misconceptions about new ideas\\nis about the ratio of question to answer in their composition.\\nPeople think big ideas are answers, but often the real insight was\\nin the question.Part of the reason we underrate questions is the way they\\'re used\\nin schools. In schools they tend to exist only briefly before being\\nanswered, like unstable particles. But a really good question can\\nbe much more than that. A really good question is a partial discovery.\\nHow do new species arise? Is the force that makes objects fall to\\nearth the same as the one that keeps planets in their orbits? By\\neven asking such questions you were already in excitingly novel\\nterritory.Unanswered questions can be uncomfortable things to carry around\\nwith you. But the more you\\'re carrying, the greater the chance of\\nnoticing a solution — or perhaps even more excitingly, noticing\\nthat two unanswered questions are the same.Sometimes you carry a question for a long time. Great work often\\ncomes from returning to a question you first noticed years before\\n— in your childhood, even — and couldn\\'t stop thinking about.\\nPeople talk a lot about the importance of keeping your youthful\\ndreams alive, but it\\'s just as important to keep your youthful\\nquestions alive.\\n[19]This is one of the places where actual expertise differs most from\\nthe popular picture of it. In the popular picture, experts are\\ncertain. But actually the more puzzled you are, the better, so long\\nas (a) the things you\\'re puzzled about matter, and (b) no one else\\nunderstands them either.Think about what\\'s happening at the moment just before a new idea\\nis discovered. Often someone with sufficient expertise is puzzled\\nabout something. Which means that originality consists partly of\\npuzzlement — of confusion! You have to be comfortable enough with\\nthe world being full of puzzles that you\\'re willing to see them,\\nbut not so comfortable that you don\\'t want to solve them.\\n[20]It\\'s a great thing to be rich in unanswered questions. And this is\\none of those situations where the rich get richer, because the best\\nway to acquire new questions is to try answering existing ones.\\nQuestions don\\'t just lead to answers, but also to more questions.\\nThe best questions grow in the answering. You notice a thread\\nprotruding from the current paradigm and try pulling on it, and it\\njust gets longer and longer. So don\\'t require a question to be\\nobviously big before you try answering it. You can rarely predict\\nthat. It\\'s hard enough even to notice the thread, let alone to\\npredict how much will unravel if you pull on it.It\\'s better to be promiscuously curious — to pull a little bit on\\na lot of threads, and see what happens. Big things start small. The\\ninitial versions of big things were often just experiments, or side\\nprojects, or talks, which then grew into something bigger. So start\\nlots of small things.Being prolific is underrated. The more different things you try,\\nthe greater the chance of discovering something new. Understand,\\nthough, that trying lots of things will mean trying lots of things\\nthat don\\'t work. You can\\'t have a lot of good ideas without also\\nhaving a lot of bad ones.\\n[21]Though it sounds more responsible to begin by studying everything\\nthat\\'s been done before, you\\'ll learn faster and have more fun by\\ntrying stuff. And you\\'ll understand previous work better when you\\ndo look at it. So err on the side of starting. Which is easier when\\nstarting means starting small; those two ideas fit together like\\ntwo puzzle pieces.How do you get from starting small to doing something great? By\\nmaking successive versions. Great things are almost always made in\\nsuccessive versions. You start with something small and evolve it,\\nand the final version is both cleverer and more ambitious than\\nanything you could have planned.It\\'s particularly useful to make successive versions when you\\'re\\nmaking something for people — to get an initial version in front\\nof them quickly, and then evolve it based on their response.Begin by trying the simplest thing that could possibly work.\\nSurprisingly often, it does. If it doesn\\'t, this will at least get\\nyou started.Don\\'t try to cram too much new stuff into any one version. There\\nare names for doing this with the first version (taking too long\\nto ship) and the second (the second system effect), but these are\\nboth merely instances of a more general principle.An early version of a new project will sometimes be dismissed as a\\ntoy. It\\'s a good sign when people do this. That means it has\\neverything a new idea needs except scale, and that tends to follow.\\n[22]The alternative to starting with something small and evolving it\\nis to plan in advance what you\\'re going to do. And planning does\\nusually seem the more responsible choice. It sounds more organized\\nto say \"we\\'re going to do x and then y and then z\" than \"we\\'re going\\nto try x and see what happens.\" And it is more organized; it just\\ndoesn\\'t work as well.Planning per se isn\\'t good. It\\'s sometimes necessary, but it\\'s a\\nnecessary evil — a response to unforgiving conditions. It\\'s something\\nyou have to do because you\\'re working with inflexible media, or\\nbecause you need to coordinate the efforts of a lot of people. If\\nyou keep projects small and use flexible media, you don\\'t have to\\nplan as much, and your designs can evolve instead.\\nTake as much risk as you can afford. In an efficient market, risk\\nis proportionate to reward, so don\\'t look for certainty, but for a\\nbet with high expected value. If you\\'re not failing occasionally,\\nyou\\'re probably being too conservative.Though conservatism is usually associated with the old, it\\'s the\\nyoung who tend to make this mistake. Inexperience makes them fear\\nrisk, but it\\'s when you\\'re young that you can afford the most.Even a project that fails can be valuable. In the process of working\\non it, you\\'ll have crossed territory few others have seen, and\\nencountered questions few others have asked. And there\\'s probably\\nno better source of questions than the ones you encounter in trying\\nto do something slightly too hard.\\nUse the advantages of youth when you have them, and the advantages\\nof age once you have those. The advantages of youth are energy,\\ntime, optimism, and freedom. The advantages of age are knowledge,\\nefficiency, money, and power. With effort you can acquire some of\\nthe latter when young and keep some of the former when old.The old also have the advantage of knowing which advantages they\\nhave. The young often have them without realizing it. The biggest\\nis probably time. The young have no idea how rich they are in time.\\nThe best way to turn this time to advantage is to use it in slightly\\nfrivolous ways: to learn about something you don\\'t need to know\\nabout, just out of curiosity, or to try building something just\\nbecause it would be cool, or to become freakishly good at something.That \"slightly\" is an important qualification. Spend time lavishly\\nwhen you\\'re young, but don\\'t simply waste it. There\\'s a big difference\\nbetween doing something you worry might be a waste of time and doing\\nsomething you know for sure will be. The former is at least a bet,\\nand possibly a better one than you think.\\n[23]The most subtle advantage of youth, or more precisely of inexperience,\\nis that you\\'re seeing everything with fresh eyes. When your brain\\nembraces an idea for the first time, sometimes the two don\\'t fit\\ntogether perfectly. Usually the problem is with your brain, but\\noccasionally it\\'s with the idea. A piece of it sticks out awkwardly\\nand jabs you when you think about it. People who are used to the\\nidea have learned to ignore it, but you have the opportunity not\\nto.\\n[24]So when you\\'re learning about something for the first time, pay\\nattention to things that seem wrong or missing. You\\'ll be tempted\\nto ignore them, since there\\'s a 99% chance the problem is with you.\\nAnd you may have to set aside your misgivings temporarily to keep\\nprogressing. But don\\'t forget about them. When you\\'ve gotten further\\ninto the subject, come back and check if they\\'re still there. If\\nthey\\'re still viable in the light of your present knowledge, they\\nprobably represent an undiscovered idea.\\nOne of the most valuable kinds of knowledge you get from experience\\nis to know what you don\\'t have to worry about. The young know all\\nthe things that could matter, but not their relative importance.\\nSo they worry equally about everything, when they should worry much\\nmore about a few things and hardly at all about the rest.But what you don\\'t know is only half the problem with inexperience.\\nThe other half is what you do know that ain\\'t so. You arrive at\\nadulthood with your head full of nonsense — bad habits you\\'ve\\nacquired and false things you\\'ve been taught — and you won\\'t be\\nable to do great work till you clear away at least the nonsense in\\nthe way of whatever type of work you want to do.Much of the nonsense left in your head is left there by schools.\\nWe\\'re so used to schools that we unconsciously treat going to school\\nas identical with learning, but in fact schools have all sorts of\\nstrange qualities that warp our ideas about learning and thinking.For example, schools induce passivity. Since you were a small child,\\nthere was an authority at the front of the class telling all of you\\nwhat you had to learn and then measuring whether you did. But neither\\nclasses nor tests are intrinsic to learning; they\\'re just artifacts\\nof the way schools are usually designed.The sooner you overcome this passivity, the better. If you\\'re still\\nin school, try thinking of your education as your project, and your\\nteachers as working for you rather than vice versa. That may seem\\na stretch, but it\\'s not merely some weird thought experiment. It\\'s\\nthe truth economically, and in the best case it\\'s the truth\\nintellectually as well. The best teachers don\\'t want to be your\\nbosses. They\\'d prefer it if you pushed ahead, using them as a source\\nof advice, rather than being pulled by them through the material.Schools also give you a misleading impression of what work is like.\\nIn school they tell you what the problems are, and they\\'re almost\\nalways soluble using no more than you\\'ve been taught so far. In\\nreal life you have to figure out what the problems are, and you\\noften don\\'t know if they\\'re soluble at all.But perhaps the worst thing schools do to you is train you to win\\nby hacking the test. You can\\'t do great work by doing that. You\\ncan\\'t trick God. So stop looking for that kind of shortcut. The way\\nto beat the system is to focus on problems and solutions that others\\nhave overlooked, not to skimp on the work itself.\\nDon\\'t think of yourself as dependent on some gatekeeper giving you\\na \"big break.\" Even if this were true, the best way to get it would\\nbe to focus on doing good work rather than chasing influential\\npeople.And don\\'t take rejection by committees to heart. The qualities that\\nimpress admissions officers and prize committees are quite different\\nfrom those required to do great work. The decisions of selection\\ncommittees are only meaningful to the extent that they\\'re part of\\na feedback loop, and very few are.\\nPeople new to a field will often copy existing work. There\\'s nothing\\ninherently bad about that. There\\'s no better way to learn how\\nsomething works than by trying to reproduce it. Nor does\\ncopying necessarily make your work unoriginal. Originality is the\\npresence of new ideas, not the absence of old ones.There\\'s a good way to copy and a bad way. If you\\'re going to copy\\nsomething, do it openly instead of furtively, or worse still,\\nunconsciously. This is what\\'s meant by the famously misattributed\\nphrase \"Great artists steal.\" The really dangerous kind of copying,\\nthe kind that gives copying a bad name, is the kind that\\'s done\\nwithout realizing it, because you\\'re nothing more than a train\\nrunning on tracks laid down by someone else. But at the other\\nextreme, copying can be a sign of superiority rather than subordination.\\n[25]In many fields it\\'s almost inevitable that your early work will be\\nin some sense based on other people\\'s. Projects rarely arise in a\\nvacuum. They\\'re usually a reaction to previous work. When you\\'re\\nfirst starting out, you don\\'t have any previous work; if you\\'re\\ngoing to react to something, it has to be someone else\\'s. Once\\nyou\\'re established, you can react to your own. But while the former\\ngets called derivative and the latter doesn\\'t, structurally the two\\ncases are more similar than they seem.Oddly enough, the very novelty of the most novel ideas sometimes\\nmakes them seem at first to be more derivative than they are. New\\ndiscoveries often have to be conceived initially as variations of\\nexisting things, even by their discoverers, because there isn\\'t\\nyet the conceptual vocabulary to express them.There are definitely some dangers to copying, though. One is that\\nyou\\'ll tend to copy old things — things that were in their day at\\nthe frontier of knowledge, but no longer are.And when you do copy something, don\\'t copy every feature of it.\\nSome will make you ridiculous if you do. Don\\'t copy the manner of\\nan eminent 50 year old professor if you\\'re 18, for example, or the\\nidiom of a Renaissance poem hundreds of years later.Some of the features of things you admire are flaws they succeeded\\ndespite. Indeed, the features that are easiest to imitate are the\\nmost likely to be the flaws.This is particularly true for behavior. Some talented people are\\njerks, and this sometimes makes it seem to the inexperienced that\\nbeing a jerk is part of being talented. It isn\\'t; being talented\\nis merely how they get away with it.One of the most powerful kinds of copying is to copy something from\\none field into another. History is so full of chance discoveries\\nof this type that it\\'s probably worth giving chance a hand by\\ndeliberately learning about other kinds of work. You can take ideas\\nfrom quite distant fields if you let them be metaphors.Negative examples can be as inspiring as positive ones. In fact you\\ncan sometimes learn more from things done badly than from things\\ndone well; sometimes it only becomes clear what\\'s needed when it\\'s\\nmissing.\\nIf a lot of the best people in your field are collected in one\\nplace, it\\'s usually a good idea to visit for a while. It will\\nincrease your ambition, and also, by showing you that these people\\nare human, increase your self-confidence.\\n[26]If you\\'re earnest you\\'ll probably get a warmer welcome than you\\nmight expect. Most people who are very good at something are happy\\nto talk about it with anyone who\\'s genuinely interested. If they\\'re\\nreally good at their work, then they probably have a hobbyist\\'s\\ninterest in it, and hobbyists always want to talk about their\\nhobbies.It may take some effort to find the people who are really good,\\nthough. Doing great work has such prestige that in some places,\\nparticularly universities, there\\'s a polite fiction that everyone\\nis engaged in it. And that is far from true. People within universities\\ncan\\'t say so openly, but the quality of the work being done in\\ndifferent departments varies immensely. Some departments have people\\ndoing great work; others have in the past; others never have.\\nSeek out the best colleagues. There are a lot of projects that can\\'t\\nbe done alone, and even if you\\'re working on one that can be, it\\'s\\ngood to have other people to encourage you and to bounce ideas off.Colleagues don\\'t just affect your work, though; they also affect\\nyou. So work with people you want to become like, because you will.Quality is more important than quantity in colleagues. It\\'s better\\nto have one or two great ones than a building full of pretty good\\nones. In fact it\\'s not merely better, but necessary, judging from\\nhistory: the degree to which great work happens in clusters suggests\\nthat one\\'s colleagues often make the difference between doing great\\nwork and not.How do you know when you have sufficiently good colleagues? In my\\nexperience, when you do, you know. Which means if you\\'re unsure,\\nyou probably don\\'t. But it may be possible to give a more concrete\\nanswer than that. Here\\'s an attempt: sufficiently good colleagues\\noffer surprising insights. They can see and do things that you\\ncan\\'t. So if you have a handful of colleagues good enough to keep\\nyou on your toes in this sense, you\\'re probably over the threshold.Most of us can benefit from collaborating with colleagues, but some\\nprojects require people on a larger scale, and starting one of those\\nis not for everyone. If you want to run a project like that, you\\'ll\\nhave to become a manager, and managing well takes aptitude and\\ninterest like any other kind of work. If you don\\'t have them, there\\nis no middle path: you must either force yourself to learn management\\nas a second language, or avoid such projects.\\n[27]\\nHusband your morale. It\\'s the basis of everything when you\\'re working\\non ambitious projects. You have to nurture and protect it like a\\nliving organism.Morale starts with your view of life. You\\'re more likely to do great\\nwork if you\\'re an optimist, and more likely to if you think of\\nyourself as lucky than if you think of yourself as a victim.Indeed, work can to some extent protect you from your problems. If\\nyou choose work that\\'s pure, its very difficulties will serve as a\\nrefuge from the difficulties of everyday life. If this is escapism,\\nit\\'s a very productive form of it, and one that has been used by\\nsome of the greatest minds in history.Morale compounds via work: high morale helps you do good work, which\\nincreases your morale and helps you do even better work. But this\\ncycle also operates in the other direction: if you\\'re not doing\\ngood work, that can demoralize you and make it even harder to. Since\\nit matters so much for this cycle to be running in the right\\ndirection, it can be a good idea to switch to easier work when\\nyou\\'re stuck, just so you start to get something done.One of the biggest mistakes ambitious people make is to allow\\nsetbacks to destroy their morale all at once, like a balloon bursting.\\nYou can inoculate yourself against this by explicitly considering\\nsetbacks a part of your process. Solving hard problems always\\ninvolves some backtracking.Doing great work is a depth-first search whose root node is the\\ndesire to. So \"If at first you don\\'t succeed, try, try again\" isn\\'t\\nquite right. It should be: If at first you don\\'t succeed, either\\ntry again, or backtrack and then try again.\"Never give up\" is also not quite right. Obviously there are times\\nwhen it\\'s the right choice to eject. A more precise version would\\nbe: Never let setbacks panic you into backtracking more than you\\nneed to. Corollary: Never abandon the root node.It\\'s not necessarily a bad sign if work is a struggle, any more\\nthan it\\'s a bad sign to be out of breath while running. It depends\\nhow fast you\\'re running. So learn to distinguish good pain from\\nbad. Good pain is a sign of effort; bad pain is a sign of damage.\\nAn audience is a critical component of morale. If you\\'re a scholar,\\nyour audience may be your peers; in the arts, it may be an audience\\nin the traditional sense. Either way it doesn\\'t need to be big.\\nThe value of an audience doesn\\'t grow anything like linearly with\\nits size. Which is bad news if you\\'re famous, but good news if\\nyou\\'re just starting out, because it means a small but dedicated\\naudience can be enough to sustain you. If a handful of people\\ngenuinely love what you\\'re doing, that\\'s enough.To the extent you can, avoid letting intermediaries come between\\nyou and your audience. In some types of work this is inevitable,\\nbut it\\'s so liberating to escape it that you might be better off\\nswitching to an adjacent type if that will let you go direct.\\n[28]The people you spend time with will also have a big effect on your\\nmorale. You\\'ll find there are some who increase your energy and\\nothers who decrease it, and the effect someone has is not always\\nwhat you\\'d expect. Seek out the people who increase your energy and\\navoid those who decrease it. Though of course if there\\'s someone\\nyou need to take care of, that takes precedence.Don\\'t marry someone who doesn\\'t understand that you need to work,\\nor sees your work as competition for your attention. If you\\'re\\nambitious, you need to work; it\\'s almost like a medical condition;\\nso someone who won\\'t let you work either doesn\\'t understand you,\\nor does and doesn\\'t care.Ultimately morale is physical. You think with your body, so it\\'s\\nimportant to take care of it. That means exercising regularly,\\neating and sleeping well, and avoiding the more dangerous kinds of\\ndrugs. Running and walking are particularly good forms of exercise\\nbecause they\\'re good for thinking.\\n[29]People who do great work are not necessarily happier than everyone\\nelse, but they\\'re happier than they\\'d be if they didn\\'t. In fact,\\nif you\\'re smart and ambitious, it\\'s dangerous not to be productive.\\nPeople who are smart and ambitious but don\\'t achieve much tend to\\nbecome bitter.\\nIt\\'s ok to want to impress other people, but choose the right people.\\nThe opinion of people you respect is signal. Fame, which is the\\nopinion of a much larger group you might or might not respect, just\\nadds noise.The prestige of a type of work is at best a trailing indicator and\\nsometimes completely mistaken. If you do anything well enough,\\nyou\\'ll make it prestigious. So the question to ask about a type of\\nwork is not how much prestige it has, but how well it could be done.Competition can be an effective motivator, but don\\'t let it choose\\nthe problem for you; don\\'t let yourself get drawn into chasing\\nsomething just because others are. In fact, don\\'t let competitors\\nmake you do anything much more specific than work harder.Curiosity is the best guide. Your curiosity never lies, and it knows\\nmore than you do about what\\'s worth paying attention to.\\nNotice how often that word has come up. If you asked an oracle the\\nsecret to doing great work and the oracle replied with a single\\nword, my bet would be on \"curiosity.\"That doesn\\'t translate directly to advice. It\\'s not enough just to\\nbe curious, and you can\\'t command curiosity anyway. But you can\\nnurture it and let it drive you.Curiosity is the key to all four steps in doing great work: it will\\nchoose the field for you, get you to the frontier, cause you to\\nnotice the gaps in it, and drive you to explore them. The whole\\nprocess is a kind of dance with curiosity.\\nBelieve it or not, I tried to make this essay as short as I could.\\nBut its length at least means it acts as a filter. If you made it\\nthis far, you must be interested in doing great work. And if so\\nyou\\'re already further along than you might realize, because the\\nset of people willing to want to is small.The factors in doing great work are factors in the literal,\\nmathematical sense, and they are: ability, interest, effort, and\\nluck. Luck by definition you can\\'t do anything about, so we can\\nignore that. And we can assume effort, if you do in fact want to\\ndo great work. So the problem boils down to ability and interest.\\nCan you find a kind of work where your ability and interest will\\ncombine to yield an explosion of new ideas?Here there are grounds for optimism. There are so many different\\nways to do great work, and even more that are still undiscovered.\\nOut of all those different types of work, the one you\\'re most suited\\nfor is probably a pretty close match. Probably a comically close\\nmatch. It\\'s just a question of finding it, and how far into it your\\nability and interest can take you. And you can only answer that by\\ntrying.Many more people could try to do great work than do. What holds\\nthem back is a combination of modesty and fear. It seems presumptuous\\nto try to be Newton or Shakespeare. It also seems hard; surely if\\nyou tried something like that, you\\'d fail. Presumably the calculation\\nis rarely explicit. Few people consciously decide not to try to do\\ngreat work. But that\\'s what\\'s going on subconsciously; they shy\\naway from the question.So I\\'m going to pull a sneaky trick on you. Do you want to do great\\nwork, or not? Now you have to decide consciously. Sorry about that.\\nI wouldn\\'t have done it to a general audience. But we already know\\nyou\\'re interested.Don\\'t worry about being presumptuous. You don\\'t have to tell anyone.\\nAnd if it\\'s too hard and you fail, so what? Lots of people have\\nworse problems than that. In fact you\\'ll be lucky if it\\'s the worst\\nproblem you have.Yes, you\\'ll have to work hard. But again, lots of people have to\\nwork hard. And if you\\'re working on something you find very\\ninteresting, which you necessarily will if you\\'re on the right path,\\nthe work will probably feel less burdensome than a lot of your\\npeers\\'.The discoveries are out there, waiting to be made. Why not by you?\\nNotes[1]\\nI don\\'t think you could give a precise definition of what\\ncounts as great work. Doing great work means doing something important\\nso well that you expand people\\'s ideas of what\\'s possible. But\\nthere\\'s no threshold for importance. It\\'s a matter of degree, and\\noften hard to judge at the time anyway. So I\\'d rather people focused\\non developing their interests rather than worrying about whether\\nthey\\'re important or not. Just try to do something amazing, and\\nleave it to future generations to say if you succeeded.[2]\\nA lot of standup comedy is based on noticing anomalies in\\neveryday life. \"Did you ever notice...?\" New ideas come from doing\\nthis about nontrivial things. Which may help explain why people\\'s\\nreaction to a new idea is often the first half of laughing: Ha![3]\\nThat second qualifier is critical. If you\\'re excited about\\nsomething most authorities discount, but you can\\'t give a more\\nprecise explanation than \"they don\\'t get it,\" then you\\'re starting\\nto drift into the territory of cranks.[4]\\nFinding something to work on is not simply a matter of finding\\na match between the current version of you and a list of known\\nproblems. You\\'ll often have to coevolve with the problem. That\\'s\\nwhy it can sometimes be so hard to figure out what to work on. The\\nsearch space is huge. It\\'s the cartesian product of all possible\\ntypes of work, both known and yet to be discovered, and all possible\\nfuture versions of you.There\\'s no way you could search this whole space, so you have to\\nrely on heuristics to generate promising paths through it and hope\\nthe best matches will be clustered. Which they will not always be;\\ndifferent types of work have been collected together as much by\\naccidents of history as by the intrinsic similarities between them.[5]\\nThere are many reasons curious people are more likely to do\\ngreat work, but one of the more subtle is that, by casting a wide\\nnet, they\\'re more likely to find the right thing to work on in the\\nfirst place.[6]\\nIt can also be dangerous to make things for an audience you\\nfeel is less sophisticated than you, if that causes you to talk\\ndown to them. You can make a lot of money doing that, if you do it\\nin a sufficiently cynical way, but it\\'s not the route to great work.\\nNot that anyone using this m.o. would care.[7]\\nThis idea I learned from Hardy\\'s A Mathematician\\'s Apology,\\nwhich I recommend to anyone ambitious to do great work, in any\\nfield.[8]\\nJust as we overestimate what we can do in a day and underestimate\\nwhat we can do over several years, we overestimate the damage done\\nby procrastinating for a day and underestimate the damage done by\\nprocrastinating for several years.[9]\\nYou can\\'t usually get paid for doing exactly what you want,\\nespecially early on. There are two options: get paid for doing work\\nclose to what you want and hope to push it closer, or get paid for\\ndoing something else entirely and do your own projects on the side.\\nBoth can work, but both have drawbacks: in the first approach your\\nwork is compromised by default, and in the second you have to fight\\nto get time to do it.[10]\\nIf you set your life up right, it will deliver the focus-relax\\ncycle automatically. The perfect setup is an office you work in and\\nthat you walk to and from.[11]\\nThere may be some very unworldly people who do great work\\nwithout consciously trying to. If you want to expand this rule to\\ncover that case, it becomes: Don\\'t try to be anything except the\\nbest.[12]\\nThis gets more complicated in work like acting, where the\\ngoal is to adopt a fake persona. But even here it\\'s possible to be\\naffected. Perhaps the rule in such fields should be to avoid\\nunintentional affectation.[13]\\nIt\\'s safe to have beliefs that you treat as unquestionable\\nif and only if they\\'re also unfalsifiable. For example, it\\'s safe\\nto have the principle that everyone should be treated equally under\\nthe law, because a sentence with a \"should\" in it isn\\'t really a\\nstatement about the world and is therefore hard to disprove. And\\nif there\\'s no evidence that could disprove one of your principles,\\nthere can\\'t be any facts you\\'d need to ignore in order to preserve\\nit.[14]\\nAffectation is easier to cure than intellectual dishonesty.\\nAffectation is often a shortcoming of the young that burns off in\\ntime, while intellectual dishonesty is more of a character flaw.[15]\\nObviously you don\\'t have to be working at the exact moment\\nyou have the idea, but you\\'ll probably have been working fairly\\nrecently.[16]\\nSome say psychoactive drugs have a similar effect. I\\'m\\nskeptical, but also almost totally ignorant of their effects.[17]\\nFor example you might give the nth most important topic\\n(m-1)/m^n of your attention, for some m > 1. You couldn\\'t allocate\\nyour attention so precisely, of course, but this at least gives an\\nidea of a reasonable distribution.[18]\\nThe principles defining a religion have to be mistaken.\\nOtherwise anyone might adopt them, and there would be nothing to\\ndistinguish the adherents of the religion from everyone else.[19]\\nIt might be a good exercise to try writing down a list of\\nquestions you wondered about in your youth. You might find you\\'re\\nnow in a position to do something about some of them.[20]\\nThe connection between originality and uncertainty causes a\\nstrange phenomenon: because the conventional-minded are more certain\\nthan the independent-minded, this tends to give them the upper hand\\nin disputes, even though they\\'re generally stupider.\\n\\n   The best lack all conviction, while the worst\\n   Are full of passionate intensity.\\n\\n[21]\\nDerived from Linus Pauling\\'s \"If you want to have good ideas,\\nyou must have many ideas.\"[22]\\nAttacking a project as a \"toy\" is similar to attacking a\\nstatement as \"inappropriate.\" It means that no more substantial\\ncriticism can be made to stick.[23]\\nOne way to tell whether you\\'re wasting time is to ask if\\nyou\\'re producing or consuming. Writing computer games is less likely\\nto be a waste of time than playing them, and playing games where\\nyou create something is less likely to be a waste of time than\\nplaying games where you don\\'t.[24]\\nAnother related advantage is that if you haven\\'t said anything\\npublicly yet, you won\\'t be biased toward evidence that supports\\nyour earlier conclusions. With sufficient integrity you could achieve\\neternal youth in this respect, but few manage to. For most people,\\nhaving previously published opinions has an effect similar to\\nideology, just in quantity 1.[25]\\nIn the early 1630s Daniel Mytens made a painting of Henrietta\\nMaria handing a laurel wreath to Charles I. Van Dyck then painted\\nhis own version to show how much better he was.[26]\\nI\\'m being deliberately vague about what a place is. As of\\nthis writing, being in the same physical place has advantages that\\nare hard to duplicate, but that could change.[27]\\nThis is false when the work the other people have to do is\\nvery constrained, as with SETI@home or Bitcoin. It may be possible\\nto expand the area in which it\\'s false by defining similarly\\nrestricted protocols with more freedom of action in the nodes.[28]\\nCorollary: Building something that enables people to go around\\nintermediaries and engage directly with their audience is probably\\na good idea.[29]\\nIt may be helpful always to walk or run the same route, because\\nthat frees attention for thinking. It feels that way to me, and\\nthere is some historical evidence for it.Thanks \\nto Trevor Blackwell, Daniel Gackle, Pam Graham, Tom Howard,\\nPatrick Hsu, Steve Huffman, Jessica Livingston, Henry Lloyd-Baker,\\nBob Metcalfe, Ben Miller, Robert Morris, Michael Nielsen, Courtenay\\nPipkin, Joris Poort, Mieke Roos, Rajat Suri, Harj Taggar, Garry\\nTan, and my younger son for suggestions and for reading drafts.\\n\\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_doc_greatwork = loader.load()\n",
    "web_doc_greatwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests~=2.32.0->arxiv) (2024.2.2)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 61.4/81.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 81.3/81.3 kB 1.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=5b0ec11b37a8eab79e147da916eac373a8468876ec4420e19db54347c8aa68b5\n",
      "  Stored in directory: c:\\users\\ariji\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.11-cp38-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.11-cp38-abi3-win_amd64.whl (16.0 MB)\n",
      "   ---------------------------------------- 0.0/16.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/16.0 MB 812.7 kB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.2/16.0 MB 1.4 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/16.0 MB 2.2 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.6/16.0 MB 2.9 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/16.0 MB 3.0 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.0/16.0 MB 3.5 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.2/16.0 MB 3.2 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.4/16.0 MB 3.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.7/16.0 MB 3.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.9/16.0 MB 3.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.1/16.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.1/16.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.4/16.0 MB 3.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.8/16.0 MB 4.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.1/16.0 MB 4.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.4/16.0 MB 4.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.7/16.0 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 4.1/16.0 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.4/16.0 MB 4.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.7/16.0 MB 4.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.1/16.0 MB 5.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.4/16.0 MB 5.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.8/16.0 MB 5.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.1/16.0 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.4/16.0 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.8/16.0 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.2/16.0 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.5/16.0 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.8/16.0 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.2/16.0 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.5/16.0 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.8/16.0 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.0/16.0 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.3/16.0 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.6/16.0 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.9/16.0 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.3/16.0 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.6/16.0 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.9/16.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.3/16.0 MB 6.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.6/16.0 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 12.0/16.0 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.3/16.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.7/16.0 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.0/16.0 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.3/16.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.6/16.0 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.9/16.0 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.2/16.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.5/16.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.8/16.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.1/16.0 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.3/16.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/16.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.0/16.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.0/16.0 MB 6.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.11\n"
     ]
    }
   ],
   "source": [
    "! pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read research paper in Arxiv - ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read Arxiv document\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query=\"1706.03762\").load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query=\"1810.04805\").load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin\\nMing-Wei Chang\\nKenton Lee\\nKristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1\\nIntroduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and ﬁne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciﬁc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The ﬁne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciﬁc parameters, and is trained on the\\ndownstream tasks by simply ﬁne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches.\\nThe ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder\\nRepresentations\\nfrom\\nTransformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a “masked lan-\\nguage model” (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953).\\nThe\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\narXiv:1810.04805v2  [cs.CL]  24 May 2019\\nword based only on its context.\\nUnlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks.\\nThe code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n2\\nRelated Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\nmost widely-used approaches in this section.\\n2.1\\nUnsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiﬁcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014).\\nTo train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciﬁc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2\\nUnsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the ﬁrst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text\\n(Col-\\nlobert and Weston, 2008).\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nﬁne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch.\\nAt least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\net al., 2018a).\\nLeft-to-right language model-\\nBERT\\nBERT\\nE[CLS]\\nE1\\n E[SEP]\\n...\\nEN\\nE1’\\n...\\nEM’\\nC\\nT1\\nT[SEP]\\n...\\nTN\\nT1’\\n...\\nTM’\\n[CLS]\\nTok 1\\n [SEP]\\n...\\nTok N\\nTok 1\\n...\\nTokM\\nQuestion\\nParagraph\\nStart/End Span\\nBERT\\nE[CLS]\\nE1\\n E[SEP]\\n...\\nEN\\nE1’\\n...\\nEM’\\nC\\nT1\\nT[SEP]\\n...\\nTN\\nT1’\\n...\\nTM’\\n[CLS]\\nTok 1\\n [SEP]\\n...\\nTok N\\nTok 1\\n...\\nTokM\\nMasked Sentence A\\nMasked Sentence B\\nPre-training\\nFine-Tuning\\nNSP\\nMask LM\\nMask LM\\nUnlabeled Sentence A and B Pair \\nSQuAD\\nQuestion Answer Pair\\nNER\\nMNLI\\nFigure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3\\nTransfer Learning from Supervised Data\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to ﬁne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n3\\nBERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and ﬁne-tuning.\\nDur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks.\\nFor ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-\\nmal difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel Architecture\\nBERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorﬂow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n4We note that in the literature the bidirectional Trans-\\nInput/Output Representations\\nTo make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ⟨Question, Answer ⟩) in one token sequence.\\nThroughout this work, a “sentence” can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A “sequence” refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The ﬁrst\\ntoken of every sequence is always a special clas-\\nsiﬁcation token ([CLS]). The ﬁnal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiﬁcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the ﬁnal hidden\\nvector of the special [CLS] token as C ∈RH,\\nand the ﬁnal hidden vector for the ith input token\\nas Ti ∈RH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1\\nPre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM\\nIntuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model.\\nUnfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly “see itself”, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a “masked\\nLM” (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the ﬁnal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.\\nAlthough this allows us to obtain a bidirec-\\ntional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nﬁne-tuning, since the [MASK] token does not ap-\\npear during ﬁne-tuning. To mitigate this, we do\\nnot always replace “masked” words with the ac-\\ntual [MASK] token. The training data generator\\nchooses 15% of the token positions at random for\\nprediction. If the i-th token is chosen, we replace\\nthe i-th token with (1) the [MASK] token 80% of\\nthe time (2) a random token 10% of the time (3)\\nthe unchanged i-th token 10% of the time. Then,\\nTi will be used to predict the original token with\\ncross entropy loss. We compare variations of this\\nprocedure in Appendix C.2.\\nTask #2:\\nNext Sentence Prediction (NSP)\\nMany important downstream tasks such as Ques-\\ntion Answering (QA) and Natural Language Infer-\\nence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-\\nrectly captured by language modeling. In order\\nto train a model that understands sentence rela-\\ntionships, we pre-train for a binarized next sen-\\ntence prediction task that can be trivially gener-\\nated from any monolingual corpus. Speciﬁcally,\\nwhen choosing the sentences A and B for each pre-\\ntraining example, 50% of the time B is the actual\\nnext sentence that follows A (labeled as IsNext),\\nand 50% of the time it is a random sentence from\\nthe corpus (labeled as NotNext).\\nAs we show\\nin Figure 1, C is used for next sentence predic-\\ntion (NSP).5 Despite its simplicity, we demon-\\nstrate in Section 5.1 that pre-training towards this\\ntask is very beneﬁcial to both QA and NLI. 6\\n5The ﬁnal model achieves 97%-98% accuracy on NSP.\\n6The vector C is not a meaningful sentence representation\\nwithout ﬁne-tuning, since it was trained with NSP.\\n[CLS]\\nhe\\nlikes\\nplay\\n##ing\\n[SEP]\\nmy\\ndog\\nis\\ncute\\n[SEP]\\nInput\\nE[CLS]\\nEhe\\nElikes\\nEplay\\nE##ing\\nE[SEP]\\nEmy\\nEdog\\nEis\\nEcute\\nE[SEP]\\nToken\\nEmbeddings\\nEA\\nEB\\nEB\\nEB\\nEB\\nEB\\nEA\\nEA\\nEA\\nEA\\nEA\\nSegment\\nEmbeddings\\nE0\\nE6\\nE7\\nE8\\nE9\\nE10\\nE1\\nE2\\nE3\\nE4\\nE5\\nPosition\\nEmbeddings\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is closely related to representation-\\nlearning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee (2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT transfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti-\\ncal to use a document-level corpus rather than a\\nshufﬂed sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2\\nFine-tuning BERT\\nFine-tuning is straightforward since the self-\\nattention mechanism in the Transformer al-\\nlows BERT to model many downstream tasks—\\nwhether they involve single text or text pairs—by\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be-\\nfore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi-\\nrectional cross attention between two sentences.\\nFor each task, we simply plug in the task-\\nspeciﬁc inputs and outputs into BERT and ﬁne-\\ntune all the parameters end-to-end.\\nAt the in-\\nput, sentence A and sentence B from pre-training\\nare analogous to (1) sentence pairs in paraphras-\\ning, (2) hypothesis-premise pairs in entailment, (3)\\nquestion-passage pairs in question answering, and\\n(4) a degenerate text-∅pair in text classiﬁcation\\nor sequence tagging. At the output, the token rep-\\nresentations are fed into an output layer for token-\\nlevel tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiﬁcation, such as en-\\ntailment or sentiment analysis.\\nCompared to pre-training, ﬁne-tuning is rela-\\ntively inexpensive. All of the results in the pa-\\nper can be replicated in at most 1 hour on a sin-\\ngle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model.7 We de-\\nscribe the task-speciﬁc details in the correspond-\\ning subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4\\nExperiments\\nIn this section, we present BERT ﬁne-tuning re-\\nsults on 11 NLP tasks.\\n4.1\\nGLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col-\\nlection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo ﬁne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the ﬁnal hid-\\nden vector C ∈RH corresponding to the ﬁrst\\ninput token ([CLS]) as the aggregate representa-\\ntion. The only new parameters introduced during\\nﬁne-tuning are classiﬁcation layer weights W ∈\\nRK×H, where K is the number of labels. We com-\\npute a standard classiﬁcation loss with C and W,\\ni.e., log(softmax(CW T )).\\n7For example, the BERT SQuAD model can be trained in\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\nF1 score of 91.0%.\\n8See (10) in https://gluebenchmark.com/faq.\\nSystem\\nMNLI-(m/mm)\\nQQP\\nQNLI\\nSST-2\\nCoLA\\nSTS-B\\nMRPC\\nRTE\\nAverage\\n392k\\n363k\\n108k\\n67k\\n8.5k\\n5.7k\\n3.5k\\n2.5k\\n-\\nPre-OpenAI SOTA\\n80.6/80.1\\n66.1\\n82.3\\n93.2\\n35.0\\n81.0\\n86.0\\n61.7\\n74.0\\nBiLSTM+ELMo+Attn\\n76.4/76.1\\n64.8\\n79.8\\n90.4\\n36.0\\n73.3\\n84.9\\n56.8\\n71.0\\nOpenAI GPT\\n82.1/81.4\\n70.3\\n87.4\\n91.3\\n45.4\\n80.0\\n82.3\\n56.0\\n75.1\\nBERTBASE\\n84.6/83.4\\n71.2\\n90.5\\n93.5\\n52.1\\n85.8\\n88.9\\n66.4\\n79.6\\nBERTLARGE\\n86.7/85.9\\n72.1\\n92.7\\n94.9\\n60.5\\n86.5\\n89.3\\n70.1\\n82.1\\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).\\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-\\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\\nWe use a batch size of 32 and ﬁne-tune for 3\\nepochs over the data for all GLUE tasks. For each\\ntask, we selected the best ﬁne-tuning learning rate\\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\\nAdditionally, for BERTLARGE we found that ﬁne-\\ntuning was sometimes unstable on small datasets,\\nso we ran several random restarts and selected the\\nbest model on the Dev set. With random restarts,\\nwe use the same pre-trained checkpoint but per-\\nform different ﬁne-tuning data shufﬂing and clas-\\nsiﬁer layer initialization.9\\nResults are presented in Table 1.\\nBoth\\nBERTBASE and BERTLARGE outperform all sys-\\ntems on all tasks by a substantial margin, obtaining\\n4.5% and 7.0% respective average accuracy im-\\nprovement over the prior state of the art. Note that\\nBERTBASE and OpenAI GPT are nearly identical\\nin terms of model architecture apart from the at-\\ntention masking. For the largest and most widely\\nreported GLUE task, MNLI, BERT obtains a 4.6%\\nabsolute accuracy improvement. On the ofﬁcial\\nGLUE leaderboard10, BERTLARGE obtains a score\\nof 80.5, compared to OpenAI GPT, which obtains\\n72.8 as of the date of writing.\\nWe ﬁnd that BERTLARGE signiﬁcantly outper-\\nforms BERTBASE across all tasks, especially those\\nwith very little training data. The effect of model\\nsize is explored more thoroughly in Section 5.2.\\n4.2\\nSQuAD v1.1\\nThe\\nStanford\\nQuestion\\nAnswering\\nDataset\\n(SQuAD v1.1) is a collection of 100k crowd-\\nsourced question/answer pairs (Rajpurkar et al.,\\n2016).\\nGiven a question and a passage from\\n9The GLUE data set distribution does not include the Test\\nlabels, and we only made a single GLUE evaluation server\\nsubmission for each of BERTBASE and BERTLARGE.\\n10https://gluebenchmark.com/leaderboard\\nWikipedia containing the answer, the task is to\\npredict the answer text span in the passage.\\nAs shown in Figure 1, in the question answer-\\ning task, we represent the input question and pas-\\nsage as a single packed sequence, with the ques-\\ntion using the A embedding and the passage using\\nthe B embedding. We only introduce a start vec-\\ntor S ∈RH and an end vector E ∈RH during\\nﬁne-tuning. The probability of word i being the\\nstart of the answer span is computed as a dot prod-\\nuct between Ti and S followed by a softmax over\\nall of the words in the paragraph: Pi =\\neS·Ti\\nP\\nj eS·Tj .\\nThe analogous formula is used for the end of the\\nanswer span. The score of a candidate span from\\nposition i to position j is deﬁned as S·Ti + E·Tj,\\nand the maximum scoring span where j ≥i is\\nused as a prediction. The training objective is the\\nsum of the log-likelihoods of the correct start and\\nend positions. We ﬁne-tune for 3 epochs with a\\nlearning rate of 5e-5 and a batch size of 32.\\nTable 2 shows top leaderboard entries as well\\nas results from top published systems (Seo et al.,\\n2017; Clark and Gardner, 2018; Peters et al.,\\n2018a; Hu et al., 2018). The top results from the\\nSQuAD leaderboard do not have up-to-date public\\nsystem descriptions available,11 and are allowed to\\nuse any public data when training their systems.\\nWe therefore use modest data augmentation in\\nour system by ﬁrst ﬁne-tuning on TriviaQA (Joshi\\net al., 2017) befor ﬁne-tuning on SQuAD.\\nOur best performing system outperforms the top\\nleaderboard system by +1.5 F1 in ensembling and\\n+1.3 F1 as a single system. In fact, our single\\nBERT model outperforms the top ensemble sys-\\ntem in terms of F1 score. Without TriviaQA ﬁne-\\n11QANet is described in Yu et al. (2018), but the system\\nhas improved substantially after publication.\\nSystem\\nDev\\nTest\\nEM\\nF1\\nEM\\nF1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman\\n-\\n-\\n82.3 91.2\\n#1 Ensemble - nlnet\\n-\\n-\\n86.0 91.7\\n#2 Ensemble - QANet\\n-\\n-\\n84.5 90.5\\nPublished\\nBiDAF+ELMo (Single)\\n-\\n85.6\\n-\\n85.8\\nR.M. Reader (Ensemble)\\n81.2 87.9 82.3 88.5\\nOurs\\nBERTBASE (Single)\\n80.8 88.5\\n-\\n-\\nBERTLARGE (Single)\\n84.1 90.9\\n-\\n-\\nBERTLARGE (Ensemble)\\n85.8 91.8\\n-\\n-\\nBERTLARGE (Sgl.+TriviaQA)\\n84.2 91.1 85.1 91.8\\nBERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\\nTable 2:\\nSQuAD 1.1 results. The BERT ensemble\\nis 7x systems which use different pre-training check-\\npoints and ﬁne-tuning seeds.\\nSystem\\nDev\\nTest\\nEM\\nF1\\nEM\\nF1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman\\n86.3 89.0 86.9 89.5\\n#1 Single - MIR-MRC (F-Net)\\n-\\n-\\n74.8 78.0\\n#2 Single - nlnet\\n-\\n-\\n74.2 77.1\\nPublished\\nunet (Ensemble)\\n-\\n-\\n71.4 74.9\\nSLQA+ (Single)\\n-\\n71.4 74.4\\nOurs\\nBERTLARGE (Single)\\n78.7 81.9 80.0 83.1\\nTable 3: SQuAD 2.0 results. We exclude entries that\\nuse BERT as one of their components.\\ntuning data, we only lose 0.1-0.4 F1, still outper-\\nforming all existing systems by a wide margin.12\\n4.3\\nSQuAD v2.0\\nThe SQuAD 2.0 task extends the SQuAD 1.1\\nproblem deﬁnition by allowing for the possibility\\nthat no short answer exists in the provided para-\\ngraph, making the problem more realistic.\\nWe use a simple approach to extend the SQuAD\\nv1.1 BERT model for this task. We treat ques-\\ntions that do not have an answer as having an an-\\nswer span with start and end at the [CLS] to-\\nken. The probability space for the start and end\\nanswer span positions is extended to include the\\nposition of the [CLS] token. For prediction, we\\ncompare the score of the no-answer span: snull =\\nS·C + E·C to the score of the best non-null span\\n12The TriviaQA data we used consists of paragraphs from\\nTriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,\\nthat contain at least one of the provided possible answers.\\nSystem\\nDev\\nTest\\nESIM+GloVe\\n51.9 52.7\\nESIM+ELMo\\n59.1 59.2\\nOpenAI GPT\\n-\\n78.0\\nBERTBASE\\n81.6\\n-\\nBERTLARGE\\n86.6 86.3\\nHuman (expert)†\\n-\\n85.0\\nHuman (5 annotations)†\\n-\\n88.0\\nTable 4: SWAG Dev and Test accuracies. †Human per-\\nformance is measured with 100 samples, as reported in\\nthe SWAG paper.\\nˆ\\nsi,j = maxj≥iS·Ti + E·Tj. We predict a non-null\\nanswer when ˆ\\nsi,j > snull + τ, where the thresh-\\nold τ is selected on the dev set to maximize F1.\\nWe did not use TriviaQA data for this model. We\\nﬁne-tuned for 2 epochs with a learning rate of 5e-5\\nand a batch size of 48.\\nThe results compared to prior leaderboard en-\\ntries and top published work (Sun et al., 2018;\\nWang et al., 2018b) are shown in Table 3, exclud-\\ning systems that use BERT as one of their com-\\nponents. We observe a +5.1 F1 improvement over\\nthe previous best system.\\n4.4\\nSWAG\\nThe Situations With Adversarial Generations\\n(SWAG) dataset contains 113k sentence-pair com-\\npletion examples that evaluate grounded common-\\nsense inference (Zellers et al., 2018). Given a sen-\\ntence, the task is to choose the most plausible con-\\ntinuation among four choices.\\nWhen ﬁne-tuning on the SWAG dataset, we\\nconstruct four input sequences, each containing\\nthe concatenation of the given sentence (sentence\\nA) and a possible continuation (sentence B). The\\nonly task-speciﬁc parameters introduced is a vec-\\ntor whose dot product with the [CLS] token rep-\\nresentation C denotes a score for each choice\\nwhich is normalized with a softmax layer.\\nWe ﬁne-tune the model for 3 epochs with a\\nlearning rate of 2e-5 and a batch size of 16. Re-\\nsults are presented in Table 4. BERTLARGE out-\\nperforms the authors’ baseline ESIM+ELMo sys-\\ntem by +27.1% and OpenAI GPT by 8.3%.\\n5\\nAblation Studies\\nIn this section, we perform ablation experiments\\nover a number of facets of BERT in order to better\\nunderstand their relative importance. Additional\\nDev Set\\nTasks\\nMNLI-m QNLI MRPC SST-2 SQuAD\\n(Acc)\\n(Acc)\\n(Acc)\\n(Acc)\\n(F1)\\nBERTBASE\\n84.4\\n88.4\\n86.7\\n92.7\\n88.5\\nNo NSP\\n83.9\\n84.9\\n86.5\\n92.6\\n87.9\\nLTR & No NSP\\n82.1\\n84.3\\n77.5\\n92.1\\n77.8\\n+ BiLSTM\\n82.1\\n84.1\\n75.7\\n91.6\\n84.9\\nTable 5: Ablation over the pre-training tasks using the\\nBERTBASE architecture. “No NSP” is trained without\\nthe next sentence prediction task. “LTR & No NSP” is\\ntrained as a left-to-right LM without the next sentence\\nprediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\\ndomly initialized BiLSTM on top of the “LTR + No\\nNSP” model during ﬁne-tuning.\\nablation studies can be found in Appendix C.\\n5.1\\nEffect of Pre-training Tasks\\nWe demonstrate the importance of the deep bidi-\\nrectionality of BERT by evaluating two pre-\\ntraining objectives using exactly the same pre-\\ntraining data, ﬁne-tuning scheme, and hyperpa-\\nrameters as BERTBASE:\\nNo NSP: A bidirectional model which is trained\\nusing the “masked LM” (MLM) but without the\\n“next sentence prediction” (NSP) task.\\nLTR & No NSP: A left-context-only model which\\nis trained using a standard Left-to-Right (LTR)\\nLM, rather than an MLM. The left-only constraint\\nwas also applied at ﬁne-tuning, because removing\\nit introduced a pre-train/ﬁne-tune mismatch that\\ndegraded downstream performance. Additionally,\\nthis model was pre-trained without the NSP task.\\nThis is directly comparable to OpenAI GPT, but\\nusing our larger training dataset, our input repre-\\nsentation, and our ﬁne-tuning scheme.\\nWe ﬁrst examine the impact brought by the NSP\\ntask.\\nIn Table 5, we show that removing NSP\\nhurts performance signiﬁcantly on QNLI, MNLI,\\nand SQuAD 1.1. Next, we evaluate the impact\\nof training bidirectional representations by com-\\nparing “No NSP” to “LTR & No NSP”. The LTR\\nmodel performs worse than the MLM model on all\\ntasks, with large drops on MRPC and SQuAD.\\nFor SQuAD it is intuitively clear that a LTR\\nmodel will perform poorly at token predictions,\\nsince the token-level hidden states have no right-\\nside context. In order to make a good faith at-\\ntempt at strengthening the LTR system, we added\\na randomly initialized BiLSTM on top. This does\\nsigniﬁcantly improve results on SQuAD, but the\\nresults are still far worse than those of the pre-\\ntrained bidirectional models. The BiLSTM hurts\\nperformance on the GLUE tasks.\\nWe recognize that it would also be possible to\\ntrain separate LTR and RTL models and represent\\neach token as the concatenation of the two mod-\\nels, as ELMo does. However: (a) this is twice as\\nexpensive as a single bidirectional model; (b) this\\nis non-intuitive for tasks like QA, since the RTL\\nmodel would not be able to condition the answer\\non the question; (c) this it is strictly less powerful\\nthan a deep bidirectional model, since it can use\\nboth left and right context at every layer.\\n5.2\\nEffect of Model Size\\nIn this section, we explore the effect of model size\\non ﬁne-tuning task accuracy. We trained a number\\nof BERT models with a differing number of layers,\\nhidden units, and attention heads, while otherwise\\nusing the same hyperparameters and training pro-\\ncedure as described previously.\\nResults on selected GLUE tasks are shown in\\nTable 6. In this table, we report the average Dev\\nSet accuracy from 5 random restarts of ﬁne-tuning.\\nWe can see that larger models lead to a strict ac-\\ncuracy improvement across all four datasets, even\\nfor MRPC which only has 3,600 labeled train-\\ning examples, and is substantially different from\\nthe pre-training tasks. It is also perhaps surpris-\\ning that we are able to achieve such signiﬁcant\\nimprovements on top of models which are al-\\nready quite large relative to the existing literature.\\nFor example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018). By contrast, BERTBASE\\ncontains 110M parameters and BERTLARGE con-\\ntains 340M parameters.\\nIt has long been known that increasing the\\nmodel size will lead to continual improvements\\non large-scale tasks such as machine translation\\nand language modeling, which is demonstrated\\nby the LM perplexity of held-out training data\\nshown in Table 6.\\nHowever, we believe that\\nthis is the ﬁrst work to demonstrate convinc-\\ningly that scaling to extreme model sizes also\\nleads to large improvements on very small scale\\ntasks, provided that the model has been sufﬁ-\\nciently pre-trained. Peters et al. (2018b) presented\\nmixed results on the downstream task impact of\\nincreasing the pre-trained bi-LM size from two\\nto four layers and Melamud et al. (2016) men-\\ntioned in passing that increasing hidden dimen-\\nsion size from 200 to 600 helped, but increasing\\nfurther to 1,000 did not bring further improve-\\nments. Both of these prior works used a feature-\\nbased approach — we hypothesize that when the\\nmodel is ﬁne-tuned directly on the downstream\\ntasks and uses only a very small number of ran-\\ndomly initialized additional parameters, the task-\\nspeciﬁc models can beneﬁt from the larger, more\\nexpressive pre-trained representations even when\\ndownstream task data is very small.\\n5.3\\nFeature-based Approach with BERT\\nAll of the BERT results presented so far have used\\nthe ﬁne-tuning approach, where a simple classiﬁ-\\ncation layer is added to the pre-trained model, and\\nall parameters are jointly ﬁne-tuned on a down-\\nstream task. However, the feature-based approach,\\nwhere ﬁxed features are extracted from the pre-\\ntrained model, has certain advantages. First, not\\nall tasks can be easily represented by a Trans-\\nformer encoder architecture, and therefore require\\na task-speciﬁc model architecture to be added.\\nSecond, there are major computational beneﬁts\\nto pre-compute an expensive representation of the\\ntraining data once and then run many experiments\\nwith cheaper models on top of this representation.\\nIn this section, we compare the two approaches\\nby applying BERT to the CoNLL-2003 Named\\nEntity Recognition (NER) task (Tjong Kim Sang\\nand De Meulder, 2003). In the input to BERT, we\\nuse a case-preserving WordPiece model, and we\\ninclude the maximal document context provided\\nby the data. Following standard practice, we for-\\nmulate this as a tagging task but do not use a CRF\\nHyperparams\\nDev Set Accuracy\\n#L\\n#H #A LM (ppl) MNLI-m MRPC SST-2\\n3\\n768\\n12\\n5.84\\n77.9\\n79.8\\n88.4\\n6\\n768\\n3\\n5.24\\n80.6\\n82.2\\n90.7\\n6\\n768\\n12\\n4.68\\n81.9\\n84.8\\n91.3\\n12\\n768\\n12\\n3.99\\n84.4\\n86.7\\n92.9\\n12 1024\\n16\\n3.54\\n85.7\\n86.9\\n93.3\\n24 1024\\n16\\n3.23\\n86.6\\n87.8\\n93.7\\nTable 6:\\nAblation over BERT model size. #L = the\\nnumber of layers; #H = hidden size; #A = number of at-\\ntention heads. “LM (ppl)” is the masked LM perplexity\\nof held-out training data.\\nSystem\\nDev F1 Test F1\\nELMo (Peters et al., 2018a)\\n95.7\\n92.2\\nCVT (Clark et al., 2018)\\n-\\n92.6\\nCSE (Akbik et al., 2018)\\n-\\n93.1\\nFine-tuning approach\\nBERTLARGE\\n96.6\\n92.8\\nBERTBASE\\n96.4\\n92.4\\nFeature-based approach (BERTBASE)\\nEmbeddings\\n91.0\\n-\\nSecond-to-Last Hidden\\n95.6\\n-\\nLast Hidden\\n94.9\\n-\\nWeighted Sum Last Four Hidden\\n95.9\\n-\\nConcat Last Four Hidden\\n96.1\\n-\\nWeighted Sum All 12 Layers\\n95.5\\n-\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\nlayer in the output. We use the representation of\\nthe ﬁrst sub-token as the input to the token-level\\nclassiﬁer over the NER label set.\\nTo ablate the ﬁne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without ﬁne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classiﬁcation layer.\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind ﬁne-tuning the entire model. This\\ndemonstrates that BERT is effective for both ﬁne-\\ntuning and feature-based approaches.\\n6\\nConclusion\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to beneﬁt from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these ﬁndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\nReferences\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nlabeling. In Proceedings of the 27th International\\nConference on Computational Linguistics, pages\\n1638–1649.\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018.\\nCharacter-level lan-\\nguage modeling with deeper self-attention.\\narXiv\\npreprint arXiv:1808.04444.\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817–1853.\\nLuisa Bentivogli,\\nBernardo Magnini,\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe ﬁfth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120–128. Association for Computa-\\ntional Linguistics.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural language.\\nComputational linguistics, 18(4):467–479.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017.\\nSemeval-2017\\ntask 1: Semantic textual similarity multilingual and\\ncrosslingual focused evaluation.\\nIn Proceedings\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1–14, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\nQuora question pairs.\\nChristopher Clark and Matt Gardner. 2018.\\nSimple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nning, and Quoc Le. 2018.\\nSemi-supervised se-\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914–\\n1925.\\nRonan Collobert and Jason Weston. 2008. A uniﬁed\\narchitecture for natural language processing: Deep\\nneural networks with multitask learning.\\nIn Pro-\\nceedings of the 25th international conference on\\nMachine learning, pages 160–167. ACM.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc\\nBarrault, and Antoine Bordes. 2017.\\nSupervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670–680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079–3087.\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. 2009. ImageNet: A Large-Scale Hierarchical\\nImage Database. In CVPR09.\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via ﬁlling in\\nthe . arXiv preprint arXiv:1801.07736.\\nDan Hendrycks and Kevin Gimpel. 2016.\\nBridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nfrom unlabelled data. In Proceedings of the 2016\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model ﬁne-tuning for text classiﬁcation. In\\nACL. Association for Computational Linguistics.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nFuru Wei, and Ming Zhou. 2018.\\nReinforced\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors. In\\nAdvances in neural information processing systems,\\npages 3294–3302.\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188–1196.\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge. In\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nefﬁcient framework for learning sentence represen-\\ntations.\\nIn International Conference on Learning\\nRepresentations.\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111–3119. Curran Associates,\\nInc.\\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\\nable hierarchical distributed language model.\\nIn\\nD. Koller, D. Schuurmans, Y. Bengio, and L. Bot-\\ntou, editors, Advances in Neural Information Pro-\\ncessing Systems 21, pages 1081–1088. Curran As-\\nsociates, Inc.\\nAnkur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention\\nmodel for natural language inference. In EMNLP.\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. Glove: Global vectors for\\nword representation. In Empirical Methods in Nat-\\nural Language Processing (EMNLP), pages 1532–\\n1543.\\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\\nula, and Russell Power. 2017. Semi-supervised se-\\nquence tagging with bidirectional language models.\\nIn ACL.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018a. Deep contextualized word rep-\\nresentations. In NAACL.\\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\\nand Wen-tau Yih. 2018b.\\nDissecting contextual\\nword embeddings: Architecture and representation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n1499–1509.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018.\\nImproving language under-\\nstanding with unsupervised learning. Technical re-\\nport, OpenAI.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 2383–2392.\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\nHannaneh Hajishirzi. 2017. Bidirectional attention\\nﬂow for machine comprehension. In ICLR.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. 2013.\\nRecursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank.\\nIn Proceedings of the 2013 conference on\\nempirical methods in natural language processing,\\npages 1631–1642.\\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\\n2018.\\nU-net:\\nMachine reading comprehension\\nwith unanswerable questions.\\narXiv preprint\\narXiv:1810.06638.\\nWilson L Taylor. 1953.\\nCloze procedure:\\nA new\\ntool for measuring readability. Journalism Bulletin,\\n30(4):415–433.\\nErik F Tjong Kim Sang and Fien De Meulder.\\n2003.\\nIntroduction to the conll-2003 shared task:\\nLanguage-independent named entity recognition. In\\nCoNLL.\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\\nWord representations: A simple and general method\\nfor semi-supervised learning. In Proceedings of the\\n48th Annual Meeting of the Association for Compu-\\ntational Linguistics, ACL ’10, pages 384–394.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems, pages 6000–6010.\\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\\nPierre-Antoine Manzagol. 2008.\\nExtracting and\\ncomposing robust features with denoising autoen-\\ncoders.\\nIn Proceedings of the 25th international\\nconference on Machine learning, pages 1096–1103.\\nACM.\\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\\nGlue: A multi-task benchmark and analysis platform\\nfor natural language understanding. In Proceedings\\nof the 2018 EMNLP Workshop BlackboxNLP: An-\\nalyzing and Interpreting Neural Networks for NLP,\\npages 353–355.\\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\\ngranularity hierarchical attention fusion networks\\nfor reading comprehension and question answering.\\nIn Proceedings of the 56th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers). Association for Computational Lin-\\nguistics.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\\nman. 2018.\\nNeural network acceptability judg-\\nments. arXiv preprint arXiv:1805.12471.\\nAdina Williams, Nikita Nangia, and Samuel R Bow-\\nman. 2018.\\nA broad-coverage challenge corpus\\nfor sentence understanding through inference.\\nIn\\nNAACL.\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe,\\nMohammad Norouzi,\\nWolfgang Macherey,\\nMaxim Krikun,\\nYuan Cao,\\nQin Gao,\\nKlaus\\nMacherey, et al. 2016.\\nGoogle’s neural ma-\\nchine translation system: Bridging the gap between\\nhuman and machine translation.\\narXiv preprint\\narXiv:1609.08144.\\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\\nLipson. 2014. How transferable are features in deep\\nneural networks? In Advances in neural information\\nprocessing systems, pages 3320–3328.\\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\\nLe. 2018.\\nQANet: Combining local convolution\\nwith global self-attention for reading comprehen-\\nsion. In ICLR.\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\\nChoi. 2018. Swag: A large-scale adversarial dataset\\nfor grounded commonsense inference. In Proceed-\\nings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP).\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\\nFidler. 2015. Aligning books and movies: Towards\\nstory-like visual explanations by watching movies\\nand reading books.\\nIn Proceedings of the IEEE\\ninternational conference on computer vision, pages\\n19–27.\\nAppendix for “BERT: Pre-training of\\nDeep Bidirectional Transformers for\\nLanguage Understanding”\\nWe organize the appendix into three sections:\\n• Additional implementation details for BERT\\nare presented in Appendix A;\\n• Additional details for our experiments are\\npresented in Appendix B; and\\n• Additional ablation studies are presented in\\nAppendix C.\\nWe present additional ablation studies for\\nBERT including:\\n– Effect of Number of Training Steps; and\\n– Ablation for Different Masking Proce-\\ndures.\\nA\\nAdditional Details for BERT\\nA.1\\nIllustration of the Pre-training Tasks\\nWe provide examples of the pre-training tasks in\\nthe following.\\nMasked LM and the Masking Procedure\\nAs-\\nsuming the unlabeled sentence is\\nmy dog is\\nhairy, and during the random masking procedure\\nwe chose the 4-th token (which corresponding to\\nhairy), our masking procedure can be further il-\\nlustrated by\\n• 80% of the time: Replace the word with the\\n[MASK] token, e.g., my dog is hairy →\\nmy dog is [MASK]\\n• 10% of the time: Replace the word with a\\nrandom word, e.g., my dog is hairy →my\\ndog is apple\\n• 10% of the time:\\nKeep the word un-\\nchanged, e.g., my dog is hairy →my dog\\nis hairy. The purpose of this is to bias the\\nrepresentation towards the actual observed\\nword.\\nThe advantage of this procedure is that the\\nTransformer encoder does not know which words\\nit will be asked to predict or which have been re-\\nplaced by random words, so it is forced to keep\\na distributional contextual representation of ev-\\nery input token.\\nAdditionally, because random\\nreplacement only occurs for 1.5% of all tokens\\n(i.e., 10% of 15%), this does not seem to harm\\nthe model’s language understanding capability. In\\nSection C.2, we evaluate the impact this proce-\\ndure.\\nCompared to standard langauge model training,\\nthe masked LM only make predictions on 15% of\\ntokens in each batch, which suggests that more\\npre-training steps may be required for the model\\nBERT (Ours)\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\n...\\n...\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\nTrm\\n...\\n...\\nOpenAI GPT\\nLstm\\nELMo\\nLstm\\nLstm\\nLstm\\nLstm\\nLstm\\nLstm\\nLstm\\nLstm\\nLstm\\nLstm\\nLstm\\n T1\\nT2\\n TN\\n...\\n...\\n...\\n...\\n...\\n E1\\nE2\\n EN\\n...\\n T1\\nT2\\nTN\\n...\\n E1\\nE2\\n EN\\n...\\n T1\\nT2\\n TN\\n...\\n E1\\nE2\\n EN\\n...\\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\\nOpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach.\\nto converge. In Section C.1 we demonstrate that\\nMLM does converge marginally slower than a left-\\nto-right model (which predicts every token), but\\nthe empirical improvements of the MLM model\\nfar outweigh the increased training cost.\\nNext Sentence Prediction\\nThe next sentence\\nprediction task can be illustrated in the following\\nexamples.\\nInput = [CLS] the man went to [MASK] store [SEP]\\nhe bought a gallon [MASK] milk [SEP]\\nLabel = IsNext\\nInput = [CLS] the man [MASK] to the store [SEP]\\npenguin [MASK] are flight ##less birds [SEP]\\nLabel = NotNext\\nA.2\\nPre-training Procedure\\nTo generate each training input sequence, we sam-\\nple two spans of text from the corpus, which we\\nrefer to as “sentences” even though they are typ-\\nically much longer than single sentences (but can\\nbe shorter also). The ﬁrst sentence receives the A\\nembedding and the second receives the B embed-\\nding. 50% of the time B is the actual next sentence\\nthat follows A and 50% of the time it is a random\\nsentence, which is done for the “next sentence pre-\\ndiction” task. They are sampled such that the com-\\nbined length is ≤512 tokens. The LM masking is\\napplied after WordPiece tokenization with a uni-\\nform masking rate of 15%, and no special consid-\\neration given to partial word pieces.\\nWe train with batch size of 256 sequences (256\\nsequences * 512 tokens = 128,000 tokens/batch)\\nfor 1,000,000 steps, which is approximately 40\\nepochs over the 3.3 billion word corpus.\\nWe\\nuse Adam with learning rate of 1e-4, β1 = 0.9,\\nβ2 = 0.999, L2 weight decay of 0.01, learning\\nrate warmup over the ﬁrst 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob-\\nability of 0.1 on all layers. We use a gelu acti-\\nvation (Hendrycks and Gimpel, 2016) rather than\\nthe standard relu, following OpenAI GPT. The\\ntraining loss is the sum of the mean masked LM\\nlikelihood and the mean next sentence prediction\\nlikelihood.\\nTraining of BERTBASE was performed on 4\\nCloud TPUs in Pod conﬁguration (16 TPU chips\\ntotal).13 Training of BERTLARGE was performed\\non 16 Cloud TPUs (64 TPU chips total). Each pre-\\ntraining took 4 days to complete.\\nLonger sequences are disproportionately expen-\\nsive because attention is quadratic to the sequence\\nlength. To speed up pretraing in our experiments,\\nwe pre-train the model with sequence length of\\n128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3\\nFine-tuning Procedure\\nFor ﬁne-tuning, most model hyperparameters are\\nthe same as in pre-training, with the exception of\\nthe batch size, learning rate, and number of train-\\ning epochs. The dropout probability was always\\nkept at 0.1. The optimal hyperparameter values\\nare task-speciﬁc, but we found the following range\\nof possible values to work well across all tasks:\\n• Batch size: 16, 32\\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\\nTPU-now-offers-preemptible-pricing-and-global-\\navailability.html\\n• Learning rate (Adam): 5e-5, 3e-5, 2e-5\\n• Number of epochs: 2, 3, 4\\nWe also observed that large data sets (e.g.,\\n100k+ labeled training examples) were far less\\nsensitive to hyperparameter choice than small data\\nsets. Fine-tuning is typically very fast, so it is rea-\\nsonable to simply run an exhaustive search over\\nthe above parameters and choose the model that\\nperforms best on the development set.\\nA.4\\nComparison of BERT, ELMo ,and\\nOpenAI GPT\\nHere we studies the differences in recent popular\\nrepresentation learning models including ELMo,\\nOpenAI GPT and BERT. The comparisons be-\\ntween the model architectures are shown visually\\nin Figure 3. Note that in addition to the architec-\\nture differences, BERT and OpenAI GPT are ﬁne-\\ntuning approaches, while ELMo is a feature-based\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n• GPT uses a sentence separator ([SEP]) and\\nclassiﬁer token ([CLS]) which are only in-\\ntroduced at ﬁne-tuning time; BERT learns\\n[SEP], [CLS] and sentence A/B embed-\\ndings during pre-training.\\n• GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n1M steps with a batch size of 128,000 words.\\n• GPT used the same learning rate of 5e-5 for\\nall ﬁne-tuning experiments; BERT chooses a\\ntask-speciﬁc ﬁne-tuning learning rate which\\nperforms the best on the development set.\\nTo isolate the effect of these differences, we per-\\nform ablation experiments in Section 5.1 which\\ndemonstrate that the majority of the improvements\\nare in fact coming from the two pre-training tasks\\nand the bidirectionality they enable.\\nA.5\\nIllustrations of Fine-tuning on Different\\nTasks\\nThe illustration of ﬁne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speciﬁc\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks. In\\nthe ﬁgure, E represents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classiﬁcation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\nB\\nDetailed Experimental Setup\\nB.1\\nDetailed Descriptions for the GLUE\\nBenchmark Experiments.\\nOur\\nGLUE\\nresults\\nin\\nTable1\\nare\\nobtained\\nfrom\\nhttps://gluebenchmark.com/\\nleaderboard\\nand\\nhttps://blog.\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\nMNLI\\nMulti-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classiﬁ-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the ﬁrst one.\\nQQP\\nQuora Question Pairs is a binary classiﬁ-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\nQNLI\\nQuestion Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classiﬁcation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.\\nBERT\\nE[CLS]\\nE1\\n E[SEP]\\n...\\nEN\\nE1’\\n...\\nEM’\\nC\\nT1\\nT[SEP]\\n...\\nTN\\nT1’\\n...\\nTM’\\n[CLS]\\nTok \\n1\\n [SEP]\\n...\\nTok \\nN\\nTok \\n1\\n...\\nTok\\nM\\nQuestion\\nParagraph\\nBERT\\nE[CLS]\\nE1\\n E2\\n EN\\nC\\nT1\\n T2\\n TN\\nSingle Sentence \\n...\\n...\\nBERT\\nTok 1\\n Tok 2\\n Tok N\\n...\\n[CLS]\\nE[CLS]\\nE1\\n E2\\n EN\\nC\\nT1\\n T2\\n TN\\nSingle Sentence \\nB-PER\\nO\\nO\\n...\\n...\\nE[CLS]\\nE1\\n E[SEP]\\nClass \\nLabel\\n...\\nEN\\nE1’\\n...\\nEM’\\nC\\nT1\\nT[SEP]\\n...\\nTN\\nT1’\\n...\\nTM’\\nStart/End Span\\nClass \\nLabel\\nBERT\\nTok 1\\n Tok 2\\n Tok N\\n...\\n[CLS]\\nTok 1\\n[CLS]\\n[CLS]\\nTok \\n1\\n [SEP]\\n...\\nTok \\nN\\nTok \\n1\\n...\\nTok\\nM\\nSentence 1\\n...\\nSentence 2\\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\nSST-2\\nThe Stanford Sentiment Treebank is a\\nbinary single-sentence classiﬁcation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\nCoLA\\nThe Corpus of Linguistic Acceptability is\\na binary single-sentence classiﬁcation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically “acceptable” or not (Warstadt\\net al., 2018).\\nSTS-B\\nThe Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\nMRPC\\nMicrosoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\nRTE\\nRecognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\nWNLI\\nWinograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that’s been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n14Note that we only report single-task ﬁne-tuning results\\nin this paper. A multitask ﬁne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n15https://gluebenchmark.com/faq\\njority class.\\nC\\nAdditional Ablation Studies\\nC.1\\nEffect of Number of Training Steps\\nFigure 5 presents MNLI Dev accuracy after ﬁne-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n1. Question:\\nDoes BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh ﬁne-tuning accuracy?\\nAnswer: Yes, BERTBASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\nC.2\\nAblation for Different Masking\\nProcedures\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n200\\n400\\n600\\n800\\n1,000\\n76\\n78\\n80\\n82\\n84\\nPre-training Steps (Thousands)\\nMNLI Dev Accuracy\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after ﬁne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.\\nNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand ﬁne-tuning, as the [MASK] symbol never ap-\\npears during the ﬁne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both ﬁne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npliﬁed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\nMasking Rates\\nDev Set Results\\nMASK SAME\\nRND\\nMNLI\\nNER\\nFine-tune Fine-tune Feature-based\\n80%\\n10%\\n10%\\n84.2\\n95.4\\n94.9\\n100%\\n0%\\n0%\\n84.3\\n94.9\\n94.0\\n80%\\n0%\\n20%\\n84.1\\n95.2\\n94.6\\n80%\\n20%\\n0%\\n84.4\\n95.2\\n94.7\\n0%\\n20%\\n80%\\n83.7\\n94.8\\n94.6\\n0%\\n0% 100%\\n83.6\\n94.9\\n94.6\\nTable 8: Ablation over different masking strategies.\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speciﬁc strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\nFrom the table it can be seen that ﬁne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ariji\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11704 sha256=4ec0ed882f20e7be56df945b64ac322f10ef4ff2b5b4cfbfaeb2c3f7d0e067f6\n",
      "  Stored in directory: c:\\users\\ariji\\appdata\\local\\pip\\cache\\wheels\\8f\\ab\\cb\\45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read documents from wikipedia - WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read documents from wikipedia\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "document = WikipediaLoader(query=\"Football Worldcup 2022\" , load_max_docs=2).load()\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrennial world championship for national football teams organized by FIFA. It took place in Qatar from 20 November to 18 December 2022, after the country was awarded the hosting rights in 2010. It was the first World Cup to be held in the Middle East and Persian Gulf countries, and the second held entirely in Asia after the 2002 tournament in South Korea and Japan.\\nThis tournament was the last with 32 participating teams, with the number of teams being increased to 48 for the 2026 edition. To avoid the extremes of Qatar's hot climate, the event was held in November and December instead of during the traditional months of May, June, or July. It was held over a reduced time frame of 29 days with 64 matches played in eight venues across five cities. Qatar entered the event—their first World Cup—automatically as the host's national team, alongside 31 teams determined by the qualification process.\\nArgentina were crowned the champions after winning the final against the title holder France 4–2 on penalties following a 3–3 draw after extra time. It was Argentina's third title and their first since 1986, as well as being the first nation from outside of Europe to win the tournament since 2002. French player Kylian Mbappé became the first player to score a hat-trick in a World Cup final since Geoff Hurst in the 1966 final and won the Golden Boot as he scored the most goals (eight) during the tournament. Mbappé also became the first player to score in two consecutive finals since Vavá of Brazil did the same in 1958 and 1962.\\nArgentine captain Lionel Messi was voted the tournament's best player, winning his second Golden Ball. The tournament has been considered exceptionally poetic as the capstone of his career, with the win fulfilling for some commentators a previously unmet criterion to be regarded as the greatest player of all time. Teammates Emiliano Martínez and Enzo Fernández won the Golden Glove, awarded to the tournament's best goalkeeper; and the Young Player Award, awarded to the tournament's best young player, respectively. With 172 goals, the tournament set a record for the highest number of goals scored in the 32-team format, with every participating team scoring at least one goal.\\nThe choice to host the World Cup in Qatar attracted significant criticism, with concerns raised over the country's treatment of migrant workers, women, and members of the LGBT community, as well as Qatar's climate, lack of a strong football culture, scheduling changes, and allegations of bribery for hosting rights and wider FIFA corruption.\\n\\n\\n== Format ==\\nThe FIFA World Cup is a professional football tournament held between national football teams, organised by FIFA. The tournament, held every four years, was first played in 1930 in Uruguay, and has been contested by 32 teams since the 1998 event. The tournament was contested with eight round-robin groups followed by a knockout round for 16 teams. The defending champions were France, who defeated Croatia 4–2 in the 2018 FIFA World Cup Final. The event was scheduled to take place under a reduced length, from 20 November to 18 December in Qatar. Being held in Qatar, it was the first World Cup tournament to be held in the Arab world. Spectators were not required to follow most COVID-19 restrictions such as social distancing, wearing masks, and negative tests.\\n\\n\\n=== Schedule ===\\nUnlike previous FIFA World Cups, which are typically played in June and July, because of Qatar's intense summer heat and often fairly high humidity, the 2022 World Cup was played in November and December. As a result, the World Cup was unusually staged in the middle of the seasons of many domestic association football leagues, which started in late July or August, including all of the major European leagues, which had been obliged to incorporate extended breaks into their domestic schedules to accommodate the World Cup. Major European competitions had scheduled their \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The India national under-20 football team represents India at all international under-20 tournaments. They act as the main feeder team for the India U-23 and the senior Indian football team.\\nThis team is for Indian players aged under 19 at the start of the calendar year in which a two-year AFC U-19 Championship campaign begins, and as such, some players can remain with the squad until the age of 21. As long as they are eligible, players can play for India at any level, making it possible to play for the U19s first, then the senior side, and again back to playing for the U19s.\\n\\n\\n== History ==\\nThe team competes for the Asian Championship, with the finals every even-numbered year, formerly odd-numbered years. It is also eligible to participate in the SAFF U-20 Championship and the FIFA U-20 World Cup held every two years.\\nBeginning from 2024-25 I-league season, the team is also eligible to participate in the I-league, the second division of Indian Football League System. \\nPreviously, the team has participated in the I-league, then first and now second division of Indian football league system, since 2010 under the names of AIFF XI, Palian Arrows and Indian Arrows. The team was disbanded in 2021 and revived as India U-20 in 2024.\\n\\n\\n=== 1959−1970 ===\\nFor the first four seasons of AFC U-19 Championship, from 1959 to 1962 the Indian team did not enter into the tournament though there was no qualification round. India first participated in the AFC U-19 Championships in 1963 but did not move ahead from the group stage. 1964 is where the first Kabui player, Basanta Kumar Kabui represented Manipur in an International event.\\nThe first best result came at the 1966 edition where the team entered in the quarter finals with 3 wins against Burma, Japan and Singapore and a defeat by China but in the quarter final they lost to Israel by 4−0. In the next edition at 1967 AFC Youth Championship India again entered in the quarter final. First a draw of 1−1 against Israel and then defeating Malaysia by 4−1 but same as 1966 they again defeated in the quarter final, this time by Indonesia by 2−6. In 1968, the team didn't move beyond group stage, and next two edition of 1969 and 1970 India did not enter in the championships.\\n\\n\\n=== 1971−1979 ===\\nIn 1971, they again reached the quarter final for the third time in AFC championships, but the fate was same as again saw a defeat, now against Japan by 0−3. But, after a gap of two more edition, it was 1974 AFC Youth Championship, where India seen the golden moment after becoming champions in AFC Under-19 Championships. First in group stage India defeat both Laos and Burma by 1−0 and a draw against Hong Kong by 2−2. Then entering in the quarter final they defeated Singapore by 1(4)−1(1), thus reached semi-finals where defeating Thailand by 2−1, for the first time India reached the final of the Championship. But the final was a draw against Iran by 2−2 thus jointly awarded both the team as the Champions. In 1975 and 1976 edition, India didn't do much well but at 1977 AFC Youth Championship they reached the quarter final but the opponent was Iran, the defending Champion who defeated India by 3−0, thus failed to qualify for the first FIFA World Youth Championship in 1977 which later came to be known as FIFA U-20 World Cup and also failed to qualify at the 1979 edition.\\n\\n\\n=== 1980−2000, the decline ===\\nThese two decades saw a decline in the performance of the Indian team. Qualification round started from 1980 AFC Youth Championship and India failed to qualify in 6 out of 11 edition of the championships and in rest 5 edition the failed to move beyond the group stage, thus also failed to qualify for the FIFA U-20 World Cup as the finalist were only to be qualified for the competition.\\n\\n\\n=== 2002−2017, rise and fall ===\\nIn the 2002, at the AFC U-19 Championship, India reached the quarter final for the 6th time, by the virtue of third-place qualifiers, but defeated heavily by South Korea, where the Korean found the Indi\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = WikipediaLoader(query=\"Langchain\" , load_max_docs=1).load()\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n== History ==\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project\\'s Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\\nIn the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.\\nIn October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.\\n\\n\\n== Capabilities ==\\nLangChain\\'s developers highlight the framework\\'s applicability to use-cases including chatbots, retrieval-augmented generation,  document summarization, and synthetic data generation.\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database to store and retrieve vector embeddings; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\\n\\n\\n== LangChain tools ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nOfficial website\\nDiscord server support hub\\nLangchain-ai on GitHub'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
